{"docstore/data": {"c53cf204-c498-4982-927d-a3a62b06cea1": {"__data__": {"id_": "c53cf204-c498-4982-927d-a3a62b06cea1", "embedding": null, "metadata": {"bbox": [{"page": 0, "page_height": 792.0, "page_width": 612.0, "x0": 86.57, "y0": 578.84, "x1": 503.27, "y1": 646.58}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"3": {"node_id": "24264475-13bb-491d-ba2f-e85d624a86e3", "node_type": "1", "metadata": {"bbox": [{"page": 0, "page_height": 792.0, "page_width": 612.0, "x0": 308.86, "y0": 348.29, "x1": 545.11, "y1": 379.18}]}, "hash": "d43cf790659b4f48142fc647f8b1110adf9e7ff99c32d1e97cd0ab647ccd9e0c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d412ac7f-8469-483d-8f0d-99c38edf6148", "node_type": "4", "metadata": {"file_name": "pp_yolo.pdf", "file_size": 500459, "creation_date": "2024-07-23", "last_modified_date": "2024-07-23"}, "hash": "2edaafdd30fe7c744e3ab970a04ef080547384dd694c611de46e66a2183187cc", "class_name": "RelatedNodeInfo"}}, "text": "Xiang Long, Kaipeng Deng, Guanzhong Wang, Yang Zhang, Qingqing Dang, \nYuan Gao, Hui Shen, Jianguo Ren, Shumin Han, Errui Ding, Shilei Wen \n{longxiang, dengkaipeng, wangguanzhong, zhangyang57, dangqingqing, \ngaoyuan18, shenhui08, v renjianguo, hanshumin, dingerrui, wenshilei}@baidu.com \nBaidu Inc. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "24264475-13bb-491d-ba2f-e85d624a86e3": {"__data__": {"id_": "24264475-13bb-491d-ba2f-e85d624a86e3", "embedding": null, "metadata": {"bbox": [{"page": 0, "page_height": 792.0, "page_width": 612.0, "x0": 308.86, "y0": 348.29, "x1": 545.11, "y1": 379.18}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "c53cf204-c498-4982-927d-a3a62b06cea1", "node_type": "1", "metadata": {"bbox": [{"page": 0, "page_height": 792.0, "page_width": 612.0, "x0": 86.57, "y0": 578.84, "x1": 503.27, "y1": 646.58}]}, "hash": "8bf90a5a4ade4e28510948bd49cd655177da0e6bf43c5fb0436fd8d6056d23d9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a6ede9e9-9049-499f-8f31-091d8c3b3ea1", "node_type": "1", "metadata": {"bbox": [{"page": 0, "page_height": 792.0, "page_width": 612.0, "x0": 50.11, "y0": 239.57, "x1": 286.37, "y1": 524.5}]}, "hash": "f961a08e5d9e8743b94aa44254d18e4ae1c1febb5058de9eefcf64d538b4925a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d412ac7f-8469-483d-8f0d-99c38edf6148", "node_type": "4", "metadata": {"file_name": "pp_yolo.pdf", "file_size": 500459, "creation_date": "2024-07-23", "last_modified_date": "2024-07-23"}, "hash": "2edaafdd30fe7c744e3ab970a04ef080547384dd694c611de46e66a2183187cc", "class_name": "RelatedNodeInfo"}}, "text": "Figure 1. Comparison of the proposed PP-YOLO and other state- \nof-the-art object detectors. PP-YOLO runs faster than YOLOv4 \nand improves mAP from 43.5% to 45.2%. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a6ede9e9-9049-499f-8f31-091d8c3b3ea1": {"__data__": {"id_": "a6ede9e9-9049-499f-8f31-091d8c3b3ea1", "embedding": null, "metadata": {"bbox": [{"page": 0, "page_height": 792.0, "page_width": 612.0, "x0": 50.11, "y0": 239.57, "x1": 286.37, "y1": 524.5}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "24264475-13bb-491d-ba2f-e85d624a86e3", "node_type": "1", "metadata": {"bbox": [{"page": 0, "page_height": 792.0, "page_width": 612.0, "x0": 308.86, "y0": 348.29, "x1": 545.11, "y1": 379.18}]}, "hash": "d43cf790659b4f48142fc647f8b1110adf9e7ff99c32d1e97cd0ab647ccd9e0c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "99a618ca-6910-4ecc-abfd-a08897c50dcf", "node_type": "1", "metadata": {"bbox": [{"page": 0, "page_height": 792.0, "page_width": 612.0, "x0": 16.34, "y0": 237.0, "x1": 36.34, "y1": 580.3}]}, "hash": "4577c44ab420bf356d0ffeca5d5bcd53c5c38e451624b95c1e1af9ec714cd8cc", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d412ac7f-8469-483d-8f0d-99c38edf6148", "node_type": "4", "metadata": {"file_name": "pp_yolo.pdf", "file_size": 500459, "creation_date": "2024-07-23", "last_modified_date": "2024-07-23"}, "hash": "2edaafdd30fe7c744e3ab970a04ef080547384dd694c611de46e66a2183187cc", "class_name": "RelatedNodeInfo"}}, "text": "Object detection is one of the most important areas in \ncomputer vision, which plays a key role in various prac- \ntical scenarios. Due to limitation of hardware, it is often \nnecessary to sacri\ufb01ce accuracy to ensure the infer speed of \nthe detector in practice. Therefore, the balance between ef- \nfectiveness and ef\ufb01ciency of object detector must be con- \nsidered. The goal of this paper is to implement an ob- \nject detector with relatively balanced effectiveness and ef- \n\ufb01ciency that can be directly applied in actual application \nscenarios, rather than propose a novel detection model. \nConsidering that YOLOv3 has been widely used in prac- \ntice, we develop a new object detector based on YOLOv3. \nWe mainly try to combine various existing tricks that al- \nmost not increase the number of model parameters and \nFLOPs, to achieve the goal of improving the accuracy of \ndetector as much as possible while ensuring that the speed \nis almost unchanged. Since all experiments in this pa- \nper are conducted based on PaddlePaddle, we call it PP- \nYOLO. By combining multiple tricks, PP-YOLO can achieve \na better balance between effectiveness (45.2% mAP) and \nef\ufb01ciency (72.9 FPS), surpassing the existing state-of-the- \nart detectors such as Ef\ufb01cientDet and YOLOv4. Source \ncode is at https://github.com/PaddlePaddle/ \nPaddleDetection. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "99a618ca-6910-4ecc-abfd-a08897c50dcf": {"__data__": {"id_": "99a618ca-6910-4ecc-abfd-a08897c50dcf", "embedding": null, "metadata": {"bbox": [{"page": 0, "page_height": 792.0, "page_width": 612.0, "x0": 16.34, "y0": 237.0, "x1": 36.34, "y1": 580.3}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "a6ede9e9-9049-499f-8f31-091d8c3b3ea1", "node_type": "1", "metadata": {"bbox": [{"page": 0, "page_height": 792.0, "page_width": 612.0, "x0": 50.11, "y0": 239.57, "x1": 286.37, "y1": 524.5}]}, "hash": "f961a08e5d9e8743b94aa44254d18e4ae1c1febb5058de9eefcf64d538b4925a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "17679c76-e1e3-404b-95f2-4c90f9461333", "node_type": "1", "metadata": {"bbox": [{"page": 0, "page_height": 792.0, "page_width": 612.0, "x0": 50.11, "y0": 78.85, "x1": 286.37, "y1": 216.98}]}, "hash": "5c88c42c578780459f89857559c067fbbfa8cdfdcd8c0e227574086128028cb9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d412ac7f-8469-483d-8f0d-99c38edf6148", "node_type": "4", "metadata": {"file_name": "pp_yolo.pdf", "file_size": 500459, "creation_date": "2024-07-23", "last_modified_date": "2024-07-23"}, "hash": "2edaafdd30fe7c744e3ab970a04ef080547384dd694c611de46e66a2183187cc", "class_name": "RelatedNodeInfo"}}, "text": "0 \n2 \n0 \n2 <br><br>g \nu \nA \n3 <br><br>] <br><br>V \nC \n. \ns \nc \n[ <br><br>3 \nv \n9 \n9 \n0 \n2 \n1 \n. \n7 \n0 \n0 \n2 \n: \nv \ni \nX \nr \na ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "17679c76-e1e3-404b-95f2-4c90f9461333": {"__data__": {"id_": "17679c76-e1e3-404b-95f2-4c90f9461333", "embedding": null, "metadata": {"bbox": [{"page": 0, "page_height": 792.0, "page_width": 612.0, "x0": 50.11, "y0": 78.85, "x1": 286.37, "y1": 216.98}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "99a618ca-6910-4ecc-abfd-a08897c50dcf", "node_type": "1", "metadata": {"bbox": [{"page": 0, "page_height": 792.0, "page_width": 612.0, "x0": 16.34, "y0": 237.0, "x1": 36.34, "y1": 580.3}]}, "hash": "4577c44ab420bf356d0ffeca5d5bcd53c5c38e451624b95c1e1af9ec714cd8cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e48927a0-e626-4817-8557-cab6665e57fb", "node_type": "1", "metadata": {"bbox": [{"page": 0, "page_height": 792.0, "page_width": 612.0, "x0": 308.86, "y0": 78.85, "x1": 545.12, "y1": 319.38}]}, "hash": "0b41bc1edf2843155d1e7c69943f73307a37b1e9233cb5e580e5a22d15096647", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d412ac7f-8469-483d-8f0d-99c38edf6148", "node_type": "4", "metadata": {"file_name": "pp_yolo.pdf", "file_size": 500459, "creation_date": "2024-07-23", "last_modified_date": "2024-07-23"}, "hash": "2edaafdd30fe7c744e3ab970a04ef080547384dd694c611de46e66a2183187cc", "class_name": "RelatedNodeInfo"}}, "text": "1. Introduction <br><br>Object detection is an important yet challenging task. \nIn the past few years, thanks to the advance of deep con- \nvolutional neural network[18, 13], object detectors have \nachieved remarkable performance[33, 21, 31, 32, 1, 22, 28, \n9, 45, 2, 5, 37, 20, 4, 15, 35]. <br><br>In particular, one stage object detectors have a good bal- \nance between speed and accuracy, and have been widely \nused in practice[27, 22, 30, 31, 32, 1]. YOLO series, \nincluding YOLOv1[30], YOLOv2[31], YOLOv3[32] and \nYOLOv4[1], is one of the most famous series. Among ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e48927a0-e626-4817-8557-cab6665e57fb": {"__data__": {"id_": "e48927a0-e626-4817-8557-cab6665e57fb", "embedding": null, "metadata": {"bbox": [{"page": 0, "page_height": 792.0, "page_width": 612.0, "x0": 308.86, "y0": 78.85, "x1": 545.12, "y1": 319.38}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "17679c76-e1e3-404b-95f2-4c90f9461333", "node_type": "1", "metadata": {"bbox": [{"page": 0, "page_height": 792.0, "page_width": 612.0, "x0": 50.11, "y0": 78.85, "x1": 286.37, "y1": 216.98}]}, "hash": "5c88c42c578780459f89857559c067fbbfa8cdfdcd8c0e227574086128028cb9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d78092a9-0535-48fc-b636-0020d3b0158b", "node_type": "1", "metadata": {"bbox": [{"page": 1, "page_height": 792.0, "page_width": 612.0, "x0": 308.86, "y0": 455.99, "x1": 545.12, "y1": 717.85}]}, "hash": "b5fa0f842ec21e07e376d54718aa7016cff1c87dcf1fcb38d986fd3eb2149883", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d412ac7f-8469-483d-8f0d-99c38edf6148", "node_type": "4", "metadata": {"file_name": "pp_yolo.pdf", "file_size": 500459, "creation_date": "2024-07-23", "last_modified_date": "2024-07-23"}, "hash": "2edaafdd30fe7c744e3ab970a04ef080547384dd694c611de46e66a2183187cc", "class_name": "RelatedNodeInfo"}}, "text": "them, the network structures of YOLO to YOLOv3 have \nrelatively large changes. YOLOv4 considers various strate- \ngies such as bag of freebies and bag of specials on the ba- \nsis of YOLOv3, which greatly improves the performance of \nthe detector. This paper introduces an improved YOLOv3 \nmodel based on PaddlePaddle (PP-YOLO). A bunch of \ntricks that almost not increase the infer time are added to \nimprove the overall performance of the model. <br><br>Unlike YOLOv4, we did not explore different backbone \nnetworks and data augmentation methods, nor did we use \nNAS to search for hyperparameters. For the backbone, we \ndirectly use the most common ResNet[13] as the backbone \nof PP-YOLO. For data augmentation, we directly used the \nmost basic MixUp [43]. One reason is that ResNet is used \nmore wildly, such that various deep learning frameworks \nhave deeply optimized for ResNet series, which will be \nmore convenient in actual deployment and will have better \ninfer speed in practical. Another reason is that the replace- \nment of backbone and data augmentation are relatively in- \ndependent factors, almost irrelevant to the tricks discussed ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d78092a9-0535-48fc-b636-0020d3b0158b": {"__data__": {"id_": "d78092a9-0535-48fc-b636-0020d3b0158b", "embedding": null, "metadata": {"bbox": [{"page": 1, "page_height": 792.0, "page_width": 612.0, "x0": 308.86, "y0": 455.99, "x1": 545.12, "y1": 717.85}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "e48927a0-e626-4817-8557-cab6665e57fb", "node_type": "1", "metadata": {"bbox": [{"page": 0, "page_height": 792.0, "page_width": 612.0, "x0": 308.86, "y0": 78.85, "x1": 545.12, "y1": 319.38}]}, "hash": "0b41bc1edf2843155d1e7c69943f73307a37b1e9233cb5e580e5a22d15096647", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "646f4d3b-f991-45cb-af5d-fd143bfc2b5d", "node_type": "1", "metadata": {"bbox": [{"page": 1, "page_height": 792.0, "page_width": 612.0, "x0": 50.11, "y0": 408.93, "x1": 286.37, "y1": 717.85}]}, "hash": "40cdec27ddcf2f37cd607953da3ba9261ad801364cb158fc40171ad7d7aba71f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d412ac7f-8469-483d-8f0d-99c38edf6148", "node_type": "4", "metadata": {"file_name": "pp_yolo.pdf", "file_size": 500459, "creation_date": "2024-07-23", "last_modified_date": "2024-07-23"}, "hash": "2edaafdd30fe7c744e3ab970a04ef080547384dd694c611de46e66a2183187cc", "class_name": "RelatedNodeInfo"}}, "text": "ization problem, including CornerNet[19], CenterNet[8], \nExtremeNet[47] and RepPoint[40]. Breaking the limitation \nimposed by hand-craft anchors, anchor-free methods show \ngreat potential for extreme object scales and aspect ratios \n[16]. The performance of some recently proposed anchor- \nfree detectors can also compete with state-of-the-art anchor- \nbased detectors. <br><br>YOLO series detectors [30, 31, 32, 1] have been widely \nused in practice, due to their excellent effectiveness and \nef\ufb01ciency. Until the writing of this paper, it has devel- \noped to YOLOv4[1]. YOLOv4 discusses a large number \nof tricks including many \u201cbag of freebies\u201d which not in- \ncrease the infer time, and several \u201cbag of specials\u201d that in- \ncrease the inference cost by a small amount but can signif- \nicantly improve the accuracy of object detection. YOLOv4 \ngreatly improves the effectiveness and ef\ufb01ciency of the \nYOLOv3[32]. This paper is also developed based on \nYOLOv3 model and also explored a lot of tricks. Unlike \nYOLOV4, we have not explored some widely studied parts \nsuch as data augmentation and backbone. Many tricks we \ndiscussed in this paper are different from YOLOV4 and the \ndetailed implementation of tricks is also different. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "646f4d3b-f991-45cb-af5d-fd143bfc2b5d": {"__data__": {"id_": "646f4d3b-f991-45cb-af5d-fd143bfc2b5d", "embedding": null, "metadata": {"bbox": [{"page": 1, "page_height": 792.0, "page_width": 612.0, "x0": 50.11, "y0": 408.93, "x1": 286.37, "y1": 717.85}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "d78092a9-0535-48fc-b636-0020d3b0158b", "node_type": "1", "metadata": {"bbox": [{"page": 1, "page_height": 792.0, "page_width": 612.0, "x0": 308.86, "y0": 455.99, "x1": 545.12, "y1": 717.85}]}, "hash": "b5fa0f842ec21e07e376d54718aa7016cff1c87dcf1fcb38d986fd3eb2149883", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7f4932ea-4ac6-47e9-bdfc-4342111a2e55", "node_type": "1", "metadata": {"bbox": [{"page": 1, "page_height": 792.0, "page_width": 612.0, "x0": 50.11, "y0": 78.85, "x1": 286.37, "y1": 375.73}]}, "hash": "2bf5f612ba6a6836afb44990c9f4eb50dca6d355cde1e281201b967f85f786e6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d412ac7f-8469-483d-8f0d-99c38edf6148", "node_type": "4", "metadata": {"file_name": "pp_yolo.pdf", "file_size": 500459, "creation_date": "2024-07-23", "last_modified_date": "2024-07-23"}, "hash": "2edaafdd30fe7c744e3ab970a04ef080547384dd694c611de46e66a2183187cc", "class_name": "RelatedNodeInfo"}}, "text": "in this paper. Since there are already a lot of works to study \nbackbone network and to explore data augmentation, we do \nnot repeat them in this paper. Searching for hyperparame- \nters using NAS often consumes more computing power, so \nthere is usually no condition to use NAS to perform a hyper- \nparameter search in each new scenario. Therefore, we still \nuse the manually set parameters following YOLOv3[32]. \nWe believe that using a better backbone network, using \nmore effective data augmentation method and using NAS \nto search for hyperparameters can further improve the per- \nformance of PP-YOLO. <br><br>The focus of this paper is how to stack some effective \ntricks that hardly affect ef\ufb01ciency to get better performance. \nMany of these tricks cannot be directly applied to the net- \nwork structure of YOLOv3, so small modi\ufb01cation is re- \nquired. Moreover, where to add tricks also needs care- \nful consideration and experiment. This paper is not in- \nIt is more \ntended to introduce a novel object detecotor. \nlike a recipe, which tell you how to build a better detec- \ntor step by step. We have found some tricks that are ef- \nfective for the YOLOv3 detector, which can save devel- \nopers\u2019 time of trial and error. The \ufb01nal PP-YOLO model \nimproves the mAP on COCO from 43.5% to 45.2% at a \nspeed faster than YOLOv4. The code and model is released \nin the PaddleDetection code-base (https://github. \ncom/PaddlePaddle/PaddleDetection). ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7f4932ea-4ac6-47e9-bdfc-4342111a2e55": {"__data__": {"id_": "7f4932ea-4ac6-47e9-bdfc-4342111a2e55", "embedding": null, "metadata": {"bbox": [{"page": 1, "page_height": 792.0, "page_width": 612.0, "x0": 50.11, "y0": 78.85, "x1": 286.37, "y1": 375.73}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "646f4d3b-f991-45cb-af5d-fd143bfc2b5d", "node_type": "1", "metadata": {"bbox": [{"page": 1, "page_height": 792.0, "page_width": 612.0, "x0": 50.11, "y0": 408.93, "x1": 286.37, "y1": 717.85}]}, "hash": "40cdec27ddcf2f37cd607953da3ba9261ad801364cb158fc40171ad7d7aba71f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5f9c19a5-bd59-48d3-854a-69543208130d", "node_type": "1", "metadata": {"bbox": [{"page": 1, "page_height": 792.0, "page_width": 612.0, "x0": 308.86, "y0": 78.85, "x1": 545.12, "y1": 441.18}]}, "hash": "d173ce222389ae1a7ea9fd29f0586d8519be73da1531698e9f49071d12b570d9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d412ac7f-8469-483d-8f0d-99c38edf6148", "node_type": "4", "metadata": {"file_name": "pp_yolo.pdf", "file_size": 500459, "creation_date": "2024-07-23", "last_modified_date": "2024-07-23"}, "hash": "2edaafdd30fe7c744e3ab970a04ef080547384dd694c611de46e66a2183187cc", "class_name": "RelatedNodeInfo"}}, "text": "Anchor-based methods are still the mainstream of object \ndetection [33, 21, 31, 32, 1, 22, 28, 9, 45, 2, 5, 37, 20, 4, 15], \nwhich evolved from early proposal based detectors, such \nas Fast R-CNN [11]. Their core idea is to introduce an- \nchor boxes, which can be viewed as pre-de\ufb01ned propos- \nIt mainly \nals, as a priori for bounding box regression. \nincludes two branches: one-stage detectors and two-stage \ndetectors[24]. A large amount of one-stage detectors in- \ncluding YOLOv2[31], YOLOv3[32], YOLOv4[1], Reti- \nnaNet [22], Re\ufb01neDet [44], Ef\ufb01centDet [35], FreeAnchor \n[45], and two-stage detectors including faster R-CNN [33] \nFPN[21], Cascade R-CNN[2], Trident-Net[20] are pro- \nposed to promote the growth of state-of-the-art perfor- \nmance in object detection continuously. Besides, anchor- \nfree detectors have recently received more and more at- \ntention. \nIn the past two years, a large number of new \nanchor-free methods have been proposed. The anchor- \nfree method actually has a long history. Earlier works \nsuch as YOLOv1[30], DenseBox[14] and UnitBox[41] can \nbe considered as early anchor-free detectors. They can \nbe divided into two types. Anchor-point based detec- \ntors perform object bounding box regression based on an- \nchor points instead of anchor boxes, including FSAF [49], \nFCOS[36], FoveaBox[17], SAPD[48]. Keypoint based de- \ntectors reformulate the object detection as keypoints local- ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5f9c19a5-bd59-48d3-854a-69543208130d": {"__data__": {"id_": "5f9c19a5-bd59-48d3-854a-69543208130d", "embedding": null, "metadata": {"bbox": [{"page": 1, "page_height": 792.0, "page_width": 612.0, "x0": 308.86, "y0": 78.85, "x1": 545.12, "y1": 441.18}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "7f4932ea-4ac6-47e9-bdfc-4342111a2e55", "node_type": "1", "metadata": {"bbox": [{"page": 1, "page_height": 792.0, "page_width": 612.0, "x0": 50.11, "y0": 78.85, "x1": 286.37, "y1": 375.73}]}, "hash": "2bf5f612ba6a6836afb44990c9f4eb50dca6d355cde1e281201b967f85f786e6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2bccbb14-c4f9-438c-8da3-a1f8b6389ace", "node_type": "1", "metadata": {"bbox": [{"page": 2, "page_height": 792.0, "page_width": 612.0, "x0": 50.11, "y0": 431.71, "x1": 545.11, "y1": 451.63}]}, "hash": "59e6b5c0560d1d05d8e47ad6ede8a0acc3a5fce09f99740ca6981ee2c285c3eb", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d412ac7f-8469-483d-8f0d-99c38edf6148", "node_type": "4", "metadata": {"file_name": "pp_yolo.pdf", "file_size": 500459, "creation_date": "2024-07-23", "last_modified_date": "2024-07-23"}, "hash": "2edaafdd30fe7c744e3ab970a04ef080547384dd694c611de46e66a2183187cc", "class_name": "RelatedNodeInfo"}}, "text": "3. Method <br><br>An one-stage anchor-based detector is normally made \nup of a backbone network, a detection neck, which is typ- \nically a feature pyramid network (FPN), and a detection \nhead for object classi\ufb01cation and localization. They are \nalso common components in most of the one-stage anchor- \nfree detectors based on anchor-point. We \ufb01rst revise the de- \ntail structure of YOLOv3 and introduce a modi\ufb01ed version \nwhich replace the backbone to ResNet50-vd-dcn, which is \nused as the basic baseline in this paper. Then we introduce \na bunch of tricks which can improve the performance of \nYOLOv3 almost without losing ef\ufb01ciency. <br><br>3.1. Architecture <br><br>Backbone The overall architecture of YOLOv3 is shown \nin Fig. 2. \nIn original YOLOv3[32], DarkNet-53 is \ufb01rst \napplied to extract feature maps at different scales. Since \nResNet[13] has been widely used and and has been stud- \nied more extensively, there are more different variants for \nselection, and it has also been better optimized by deep \nlearning frameworks. So, we replace the original backbone \nDarkNet-53 with ResNet50-vd in PP-YOLO. Considering \ndirectly replace DarkNet-53 with ResNet50-vd will hurt the \nperformance of YOLOv3 detector. We replace some con- \nvolutional layers in ResNet50-vd with deformable convo- \nlutional layers. The effectiveness of Deformable Convolu- \ntional Networks (DCN) has been veri\ufb01ed in many detection \nmodels. DCN itself will not signi\ufb01cantly increase the num- \nber of parameters and FLOPs in the model, but in practical ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2bccbb14-c4f9-438c-8da3-a1f8b6389ace": {"__data__": {"id_": "2bccbb14-c4f9-438c-8da3-a1f8b6389ace", "embedding": null, "metadata": {"bbox": [{"page": 2, "page_height": 792.0, "page_width": 612.0, "x0": 50.11, "y0": 431.71, "x1": 545.11, "y1": 451.63}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "5f9c19a5-bd59-48d3-854a-69543208130d", "node_type": "1", "metadata": {"bbox": [{"page": 1, "page_height": 792.0, "page_width": 612.0, "x0": 308.86, "y0": 78.85, "x1": 545.12, "y1": 441.18}]}, "hash": "d173ce222389ae1a7ea9fd29f0586d8519be73da1531698e9f49071d12b570d9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0795b446-fb38-4554-86a3-0a2b6303a8ff", "node_type": "1", "metadata": {"bbox": [{"page": 2, "page_height": 792.0, "page_width": 612.0, "x0": 308.86, "y0": 162.2, "x1": 545.12, "y1": 412.3}]}, "hash": "3c2337f7dd764b69412171a00d08c2d72076e22b1fc446a7340a30166cfb4ccc", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d412ac7f-8469-483d-8f0d-99c38edf6148", "node_type": "4", "metadata": {"file_name": "pp_yolo.pdf", "file_size": 500459, "creation_date": "2024-07-23", "last_modified_date": "2024-07-23"}, "hash": "2edaafdd30fe7c744e3ab970a04ef080547384dd694c611de46e66a2183187cc", "class_name": "RelatedNodeInfo"}}, "text": "Figure 2. The network architecture of YOLOv3 and inject points for PP-YOLO. Activation layers are omitted for brevity. Details are \ndescribed in Section 3.1 and Section 3.2. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0795b446-fb38-4554-86a3-0a2b6303a8ff": {"__data__": {"id_": "0795b446-fb38-4554-86a3-0a2b6303a8ff", "embedding": null, "metadata": {"bbox": [{"page": 2, "page_height": 792.0, "page_width": 612.0, "x0": 308.86, "y0": 162.2, "x1": 545.12, "y1": 412.3}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "2bccbb14-c4f9-438c-8da3-a1f8b6389ace", "node_type": "1", "metadata": {"bbox": [{"page": 2, "page_height": 792.0, "page_width": 612.0, "x0": 50.11, "y0": 431.71, "x1": 545.11, "y1": 451.63}]}, "hash": "59e6b5c0560d1d05d8e47ad6ede8a0acc3a5fce09f99740ca6981ee2c285c3eb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "67bead9e-5886-4dc4-ba75-9b6d723500cf", "node_type": "1", "metadata": {"bbox": [{"page": 2, "page_height": 792.0, "page_width": 612.0, "x0": 50.11, "y0": 78.85, "x1": 286.37, "y1": 411.44}]}, "hash": "460406655e194faa7f46f99fb862b2747f8e3f1f18952640b12c97f3ef03b836", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d412ac7f-8469-483d-8f0d-99c38edf6148", "node_type": "4", "metadata": {"file_name": "pp_yolo.pdf", "file_size": 500459, "creation_date": "2024-07-23", "last_modified_date": "2024-07-23"}, "hash": "2edaafdd30fe7c744e3ab970a04ef080547384dd694c611de46e66a2183187cc", "class_name": "RelatedNodeInfo"}}, "text": "3.2. Selection of Tricks <br><br>The various tricks we used in this paper are described \nin this section. These tricks are all already existing, which \ncoming from different works [10, 1, 42, 39, 38, 25, 12]. This \npaper does not propose an novel detection method, but just \nfocuses on combining the existing tricks to implement an \neffective and ef\ufb01cient detector. Because many tricks can- \nnot be applied to YOLOv3 directly, we need to adjust them \naccording to the its structure. \nLarger Batch Size Using a larger batch size can improve \nthe stability of training and get better results. Here we \nchange the training batch size from 64 to 192, and adjust \nthe training schedule and learning rate accordingly. \nEMA When training a model, it is often bene\ufb01cial to main- \ntain moving averages of the trained parameters. Evaluations \nthat use averaged parameters sometimes produce signi\ufb01- \ncantly better results than the \ufb01nal trained values [35]. The \nExponential Moving Average (EMA) compute the moving \naverages of trained parameters using exponential decay. For \neach parameter W , we maintain an shadow parameter ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "67bead9e-5886-4dc4-ba75-9b6d723500cf": {"__data__": {"id_": "67bead9e-5886-4dc4-ba75-9b6d723500cf", "embedding": null, "metadata": {"bbox": [{"page": 2, "page_height": 792.0, "page_width": 612.0, "x0": 50.11, "y0": 78.85, "x1": 286.37, "y1": 411.44}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "0795b446-fb38-4554-86a3-0a2b6303a8ff", "node_type": "1", "metadata": {"bbox": [{"page": 2, "page_height": 792.0, "page_width": 612.0, "x0": 308.86, "y0": 162.2, "x1": 545.12, "y1": 412.3}]}, "hash": "3c2337f7dd764b69412171a00d08c2d72076e22b1fc446a7340a30166cfb4ccc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "daf3014b-792c-4ec0-84de-d75464160b2b", "node_type": "1", "metadata": {"bbox": [{"page": 2, "page_height": 792.0, "page_width": 612.0, "x0": 308.86, "y0": 78.85, "x1": 545.11, "y1": 150.13}]}, "hash": "07a5deb1cb008766fde94b7282c1fefdfae17781c425a71159bdac5c9de338e9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d412ac7f-8469-483d-8f0d-99c38edf6148", "node_type": "4", "metadata": {"file_name": "pp_yolo.pdf", "file_size": 500459, "creation_date": "2024-07-23", "last_modified_date": "2024-07-23"}, "hash": "2edaafdd30fe7c744e3ab970a04ef080547384dd694c611de46e66a2183187cc", "class_name": "RelatedNodeInfo"}}, "text": "application, too many DCN layers will greatly increase in- \nfer time. Therefore, in order to balance the ef\ufb01ciency and \neffectiveness, we only replace 3 \u00d7 3 convolution layers in \nthe last stage with DCNs. We denote this modi\ufb01ed back- \nbone as ResNet50-vd-dcn, and the output of stage 3, 4 and \n5 as C3, C4, C5. <br><br>Detection Neck Then the FPN [21] is used to build an \nfeature pyramid with lateral connections between feature \nmaps. Feature maps C3, C4, C5 are input to the FPN mod- \nule. We denote the output feature maps of pyramid level l \nas Pl, where l = 3, 4, 5 in our experiments. The resolution \nof Pl is W \n2l for an input image of size W \u00d7 H. The \ndetail structure of FPN is shown in Fig. 2. \n2l \u00d7 H <br><br>Detection Head The detection head of YOLOv3 is very \nsimple. It consists of two convolutional layers. A 3 \u00d7 3 con- \nvolutional followed by an 1 \u00d7 1 convolutional layer is adopt \nto get the \ufb01nal predictions. The output channel of each \ufb01- \nnal prediction is 3(K + 5), where K is number of classes. \nEach position on each \ufb01nal prediction map has been asso- \nciate with three different anchors. For each anchor, the \ufb01rst \nK channels are the prediction of probability for K classes. \nThe following 4 channels are the prediction for bounding \nbox localization. The last channel is the prediction of ob- \njectness score. For classi\ufb01cation and localization, cross en- \ntropy loss and L1 loss is adopt correspondingly. An ob- \njectness loss [32] is applied to supervise objectness score, \nwhich is used to identify whether is there an object or not. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "daf3014b-792c-4ec0-84de-d75464160b2b": {"__data__": {"id_": "daf3014b-792c-4ec0-84de-d75464160b2b", "embedding": null, "metadata": {"bbox": [{"page": 2, "page_height": 792.0, "page_width": 612.0, "x0": 308.86, "y0": 78.85, "x1": 545.11, "y1": 150.13}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "67bead9e-5886-4dc4-ba75-9b6d723500cf", "node_type": "1", "metadata": {"bbox": [{"page": 2, "page_height": 792.0, "page_width": 612.0, "x0": 50.11, "y0": 78.85, "x1": 286.37, "y1": 411.44}]}, "hash": "460406655e194faa7f46f99fb862b2747f8e3f1f18952640b12c97f3ef03b836", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "076e8afd-9d4b-4a39-9b8d-d304feac8cc4", "node_type": "1", "metadata": {"bbox": [{"page": 3, "page_height": 792.0, "page_width": 612.0, "x0": 50.11, "y0": 304.86, "x1": 286.37, "y1": 717.85}]}, "hash": "19f82c632033555c2a034516876b77cff14ea0bda47dbac89ec1345b882d7641", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d412ac7f-8469-483d-8f0d-99c38edf6148", "node_type": "4", "metadata": {"file_name": "pp_yolo.pdf", "file_size": 500459, "creation_date": "2024-07-23", "last_modified_date": "2024-07-23"}, "hash": "2edaafdd30fe7c744e3ab970a04ef080547384dd694c611de46e66a2183187cc", "class_name": "RelatedNodeInfo"}}, "text": "WEM A = \u03bbWEM A + (1 \u2212 \u03bb)W,  (1) <br><br>where \u03bb is the decay. We apply EMA with decay \u03bb of \n0.9998 and use the shadow parameter WEM A for evalua- \ntion. \nDropBlock [10] DropBlock is a form of structured dropout, ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "076e8afd-9d4b-4a39-9b8d-d304feac8cc4": {"__data__": {"id_": "076e8afd-9d4b-4a39-9b8d-d304feac8cc4", "embedding": null, "metadata": {"bbox": [{"page": 3, "page_height": 792.0, "page_width": 612.0, "x0": 50.11, "y0": 304.86, "x1": 286.37, "y1": 717.85}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "daf3014b-792c-4ec0-84de-d75464160b2b", "node_type": "1", "metadata": {"bbox": [{"page": 2, "page_height": 792.0, "page_width": 612.0, "x0": 308.86, "y0": 78.85, "x1": 545.11, "y1": 150.13}]}, "hash": "07a5deb1cb008766fde94b7282c1fefdfae17781c425a71159bdac5c9de338e9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "690523a4-7491-45d9-a84a-6eddccdc2735", "node_type": "1", "metadata": {"bbox": [{"page": 3, "page_height": 792.0, "page_width": 612.0, "x0": 308.86, "y0": 268.49, "x1": 545.12, "y1": 717.85}]}, "hash": "9e26f9ea25f6c303d717ed5f44ec8549fc9a5cec00188c02c960ebb8bfe77c43", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d412ac7f-8469-483d-8f0d-99c38edf6148", "node_type": "4", "metadata": {"file_name": "pp_yolo.pdf", "file_size": 500459, "creation_date": "2024-07-23", "last_modified_date": "2024-07-23"}, "hash": "2edaafdd30fe7c744e3ab970a04ef080547384dd694c611de46e66a2183187cc", "class_name": "RelatedNodeInfo"}}, "text": "where units in a contiguous region of a feature map are \ndropped together. Different from the original paper, we \nonly apply DropBlock to the FPN, since we \ufb01nd that adding \nDropBlock to the backbone will lead to a decrease of the \nperformance. The detailed inject points of the DropBlock \nare marked by \u201dtriangles\u201d in Figure 2. \nIoU Loss [42] Bounding box regression is the crucial step in \nobject detection. In YOLOv3, L1 loss is adopted for bound- \ning box regression. It is not tailored to the mAP evaluation \nmetric, which is strongly rely on Intersection over Union \n(IoU). IoU loss and other variations such as CIoU loss and \nGIoU loss[46, 34] have been proposed to address this prob- \nlem. Different from YOLOv4, we do not replace the L1-loss \nwith IoU loss directly, we add another branch to calculate \nIoU loss. We \ufb01nd that the improvements of various IoU loss \nare similar, so we choose the most basic IoU loss [42]. \nIoU Aware [39] In YOLOv3, the classi\ufb01cation probabil- \nity and objectness score is multiplied as the \ufb01nal detection \ncon\ufb01dence, which do not consider the localization accu- \nracy. To solve this problem, an IoU prediction branch is \nadded to measure the accuracy of localization. During train- \ning, IoU aware loss is adopt to training the IoU prediction \nbranch. During inference, the predicted IoU is multiplied \nby the classi\ufb01cation probability and objectiveness score to \ncompute the \ufb01nal detection con\ufb01dence, which is more cor- \nrelated with the localization accuracy. The \ufb01nal detection \ncon\ufb01dence is then used as the input of the subsequent NMS. \nIoU aware branch will add additional computational cost. \nHowever, only 0.01% number of parameters and 0.0001% \nFLOPs are added, which can be almost ignored. \nGrid Sensitive[1] Grid Sensitive is an effective trick intro- \nduced by YOLOv4. When we decode the coordinate of the \nbounding box center x and y, in original YOLOv3, we can \nget them by ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "690523a4-7491-45d9-a84a-6eddccdc2735": {"__data__": {"id_": "690523a4-7491-45d9-a84a-6eddccdc2735", "embedding": null, "metadata": {"bbox": [{"page": 3, "page_height": 792.0, "page_width": 612.0, "x0": 308.86, "y0": 268.49, "x1": 545.12, "y1": 717.85}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "076e8afd-9d4b-4a39-9b8d-d304feac8cc4", "node_type": "1", "metadata": {"bbox": [{"page": 3, "page_height": 792.0, "page_width": 612.0, "x0": 50.11, "y0": 304.86, "x1": 286.37, "y1": 717.85}]}, "hash": "19f82c632033555c2a034516876b77cff14ea0bda47dbac89ec1345b882d7641", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "231cf635-3d41-4468-b701-640e0056f324", "node_type": "1", "metadata": {"bbox": [{"page": 3, "page_height": 792.0, "page_width": 612.0, "x0": 50.11, "y0": 78.85, "x1": 286.37, "y1": 296.61}]}, "hash": "ec2adf705ba460730ecb5467830a2df3250e4ca3e7671b9527708664c8d4fdd1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d412ac7f-8469-483d-8f0d-99c38edf6148", "node_type": "4", "metadata": {"file_name": "pp_yolo.pdf", "file_size": 500459, "creation_date": "2024-07-23", "last_modified_date": "2024-07-23"}, "hash": "2edaafdd30fe7c744e3ab970a04ef080547384dd694c611de46e66a2183187cc", "class_name": "RelatedNodeInfo"}}, "text": "creasing function of their overlaps. However, such process \nis sequential like traditional Greedy NMS and could not \nbe implemented in parallel. Matrix NMS views this pro- \ncess from another perspective and implement it in a parallel \nmanner. Therefore, the Matrix NMS is faster than tradi- \ntional NMS, which will not bring any loss of ef\ufb01ciency. \nCoordConv [25] CoordConv, which works by giving con- \nvolution access to its own input coordinates through the use \nof extra coordinate channels. CoordConv allows networks \nto learn either complete translation invariance or varying \ndegrees of translation dependence. Considering that Coord- \nConv will add two inputs channels to the convolution layer, \nsome parameters and FLOPs will be added. In order to re- \nduce the loss of ef\ufb01ciency as much as possible, we do not \nchange convolutional layers in backbone, and only replace \nthe 1x1 convolution layer in FPN and the \ufb01rst convolution \nlayer in detection head with CoordConv. The detailed in- \nject points of the CoordConv are marked by \u201ddiamonds\u201d in \nFigure 2. \nSPP [12] The Spatial Pyramid Pooling (SPP) is \ufb01rst pro- \nposed by He et al[12]. SPP integrates SPM into CNN \nand use max-pooling operation instead of bag-of-word op- \neration. YOLOv4 apply SPP module by concatenating \nmax-pooling outputs with kernel size k \u00d7 k, where k = \n{1, 5, 9, 13}, and stride equals to 1. Under this design, a \nrelatively large k \u00d7 k max-pooling effectively increase the \nreceptive \ufb01eld of backbone feature. In detail, the SPP only \napplied on the top feature map as shown in Figure 2 with \n\u201dstar\u201d mark. No parameter are introduced by SPP itself, but \nthe number of input channel of the following convolutional \nlayer will increase. So around 2% additional papameters \nand 1% extra FLOPs are introduced. \nBetter Pretrain Model Using a pretrain model with higher \nclassi\ufb01cation accuracy on ImageNet may result in better de- \ntection performance. Here we use the distilled ResNet50-vd \nmodel as the pretrain model [29] . This obviously does not \naffect the ef\ufb01ciency of the detector. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "231cf635-3d41-4468-b701-640e0056f324": {"__data__": {"id_": "231cf635-3d41-4468-b701-640e0056f324", "embedding": null, "metadata": {"bbox": [{"page": 3, "page_height": 792.0, "page_width": 612.0, "x0": 50.11, "y0": 78.85, "x1": 286.37, "y1": 296.61}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "690523a4-7491-45d9-a84a-6eddccdc2735", "node_type": "1", "metadata": {"bbox": [{"page": 3, "page_height": 792.0, "page_width": 612.0, "x0": 308.86, "y0": 268.49, "x1": 545.12, "y1": 717.85}]}, "hash": "9e26f9ea25f6c303d717ed5f44ec8549fc9a5cec00188c02c960ebb8bfe77c43", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c5ea2aae-9077-431f-96db-46706e9f7d26", "node_type": "1", "metadata": {"bbox": [{"page": 3, "page_height": 792.0, "page_width": 612.0, "x0": 308.86, "y0": 78.85, "x1": 545.12, "y1": 255.68}]}, "hash": "f9c703093d9973154aac4dfdfb45cc7390ac22971ceda1dde4cae9adcc71c04e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d412ac7f-8469-483d-8f0d-99c38edf6148", "node_type": "4", "metadata": {"file_name": "pp_yolo.pdf", "file_size": 500459, "creation_date": "2024-07-23", "last_modified_date": "2024-07-23"}, "hash": "2edaafdd30fe7c744e3ab970a04ef080547384dd694c611de46e66a2183187cc", "class_name": "RelatedNodeInfo"}}, "text": "x = s \u00b7 (gx + \u03c3(px)), \ny = s \u00b7 (gy + \u03c3(py)), <br><br>(2) <br><br>(3) <br><br>where \u03c3 is the sigmoid function, gx and gy are integers and \ns is a scale factor. Obviously, x and y cannot be exactly \nequal to s \u00b7 gx or s \u00b7 (gx + 1). This makes it dif\ufb01cult to \npredict the centres of bounding boxes that just located on \nthe grid boundary. We can address this problem, by change \nthe equation to <br><br>x = s \u00b7 (gx + \u03b1 \u00b7 \u03c3(px) \u2212 (\u03b1 \u2212 1)/2), \ny = s \u00b7 (gy + \u03b1 \u00b7 \u03c3(py) \u2212 (\u03b1 \u2212 1)/2), <br><br>(4) <br><br>(5) <br><br>where \u03b1 is set to 1.05 in this paper. This makes it easier for \nthe model to predict bounding box center exactly located on \nthe grid boundary. The FLOPs added by Grid Sensitive is \nreally small, and can be totally ignored. \nMatrix NMS [38] Matrix NMS is motivated by Soft-NMS, \nwhich decays the other detection scores as amonotonic de- ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c5ea2aae-9077-431f-96db-46706e9f7d26": {"__data__": {"id_": "c5ea2aae-9077-431f-96db-46706e9f7d26", "embedding": null, "metadata": {"bbox": [{"page": 3, "page_height": 792.0, "page_width": 612.0, "x0": 308.86, "y0": 78.85, "x1": 545.12, "y1": 255.68}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "231cf635-3d41-4468-b701-640e0056f324", "node_type": "1", "metadata": {"bbox": [{"page": 3, "page_height": 792.0, "page_width": 612.0, "x0": 50.11, "y0": 78.85, "x1": 286.37, "y1": 296.61}]}, "hash": "ec2adf705ba460730ecb5467830a2df3250e4ca3e7671b9527708664c8d4fdd1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a10b8b19-c68d-4a05-924f-80a65d4e997d", "node_type": "1", "metadata": {"bbox": [{"page": 4, "page_height": 792.0, "page_width": 612.0, "x0": 105.66, "y0": 583.95, "x1": 459.49, "y1": 719.11}]}, "hash": "a547f115f058fc36baab324df0bfa2dc0d48a33f965ad2a7bdf431d3be111fa0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d412ac7f-8469-483d-8f0d-99c38edf6148", "node_type": "4", "metadata": {"file_name": "pp_yolo.pdf", "file_size": 500459, "creation_date": "2024-07-23", "last_modified_date": "2024-07-23"}, "hash": "2edaafdd30fe7c744e3ab970a04ef080547384dd694c611de46e66a2183187cc", "class_name": "RelatedNodeInfo"}}, "text": "4. Experiment <br><br>In this section, we present the effectiveness of differ- \nent tricks. Experiments were carried out on the bounding \nbox detection track of the COCO dataset [23]. Following \nthe common practice [32, 35, 1], we use trainval35k \nsplit for training, which contains \u223c118k images, minival \nsplit (5k) for validation and ablation study, and test-dev \nsplit(\u223c20k) for testing. <br><br>4.1. Implementation Details <br><br>We use ResNet50-vd-dcn[13] as the backbone networks \nunless speci\ufb01ed. The architecture of FPN and head in our \nbasic models is completely the same as YOLOv3[32]. The \ndetails have been presented in section 3.1. We initialize ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a10b8b19-c68d-4a05-924f-80a65d4e997d": {"__data__": {"id_": "a10b8b19-c68d-4a05-924f-80a65d4e997d", "embedding": null, "metadata": {"bbox": [{"page": 4, "page_height": 792.0, "page_width": 612.0, "x0": 105.66, "y0": 583.95, "x1": 459.49, "y1": 719.11}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "c5ea2aae-9077-431f-96db-46706e9f7d26", "node_type": "1", "metadata": {"bbox": [{"page": 3, "page_height": 792.0, "page_width": 612.0, "x0": 308.86, "y0": 78.85, "x1": 545.12, "y1": 255.68}]}, "hash": "f9c703093d9973154aac4dfdfb45cc7390ac22971ceda1dde4cae9adcc71c04e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b53e6f20-453e-4bd7-8f79-1f19f8cb0965", "node_type": "1", "metadata": {"bbox": [{"page": 4, "page_height": 792.0, "page_width": 612.0, "x0": 50.11, "y0": 377.39, "x1": 286.37, "y1": 550.99}]}, "hash": "aea9f01fb7ef9dc8a15e5f07dd704aef57b8607c924b543185f3afd9c4c16ee7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d412ac7f-8469-483d-8f0d-99c38edf6148", "node_type": "4", "metadata": {"file_name": "pp_yolo.pdf", "file_size": 500459, "creation_date": "2024-07-23", "last_modified_date": "2024-07-23"}, "hash": "2edaafdd30fe7c744e3ab970a04ef080547384dd694c611de46e66a2183187cc", "class_name": "RelatedNodeInfo"}}, "text": "Methods <br><br>mAP(%) \n38.9 \n39.1 \n41.4 \n41.9 \n42.5 \n42.8 \n43.5 \n44.0 \n44.3 \n44.6  Parameters GFLOPs \n59.13 M \n43.89 M \n43.89 M \n43.89 M \n43.90 M \n43.90 M \n43.90 M \n43.93 M \n44.93 M \n44.93 M  infer time \n17.2 ms \n12.6 ms \n12.6 ms \n12.6 ms \n13.3 ms \n13.4 ms \n13.4 ms \n13.5ms \n13.7 ms \n13.7 ms  A Darknet53 YOLOv3 \nB ResNet50-vd-dcn YOLOv3 \nC B + LB + EMA + DropBlock \nD C + IoU Loss \nE D + Iou Aware \nF \nG F + Matrix NMS \nH G + CoordConv \nH + SPP \nI \nI + Better ImageNet Pretrain \nJ  65.52 \n44.71 \n44.71 \n44.71 \n44.71 \n44.71 \n44.71 \n44.76 \n45.12 \n45.12 <br><br>E + Grid Sensitive ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b53e6f20-453e-4bd7-8f79-1f19f8cb0965": {"__data__": {"id_": "b53e6f20-453e-4bd7-8f79-1f19f8cb0965", "embedding": null, "metadata": {"bbox": [{"page": 4, "page_height": 792.0, "page_width": 612.0, "x0": 50.11, "y0": 377.39, "x1": 286.37, "y1": 550.99}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "a10b8b19-c68d-4a05-924f-80a65d4e997d", "node_type": "1", "metadata": {"bbox": [{"page": 4, "page_height": 792.0, "page_width": 612.0, "x0": 105.66, "y0": 583.95, "x1": 459.49, "y1": 719.11}]}, "hash": "a547f115f058fc36baab324df0bfa2dc0d48a33f965ad2a7bdf431d3be111fa0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7fefe3ba-c364-4a08-90e3-6d5b8d548ffa", "node_type": "1", "metadata": {"bbox": [{"page": 4, "page_height": 792.0, "page_width": 612.0, "x0": 308.86, "y0": 331.18, "x1": 545.12, "y1": 550.99}]}, "hash": "1cd5a4cce42e5e75a9b258d38c916bd5d7724096bd252e7a43e955af3726ceef", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d412ac7f-8469-483d-8f0d-99c38edf6148", "node_type": "4", "metadata": {"file_name": "pp_yolo.pdf", "file_size": 500459, "creation_date": "2024-07-23", "last_modified_date": "2024-07-23"}, "hash": "2edaafdd30fe7c744e3ab970a04ef080547384dd694c611de46e66a2183187cc", "class_name": "RelatedNodeInfo"}}, "text": "our detectors following common practice. Speci\ufb01cally, our \nbackbone networks are initialized with the weights pre- \ntrained on ImageNet[7]. For the FPN and detection heads, \nwe initialize them randomly as same as in YOLOv3[32]. \nFor the baseline model (A, B), The training schedule is as \nsame as YOLOv3. Under larger batch size setting, the entire \nnetwork is trained with stochastic gradient descent (SGD) \nfor 250K iterations with the initial learning rate being 0.01 \nand a minibatch of 192 images distributed on 8 GPUs. The \nlearning rate is divided by 10 at iteration 150K and 200K, \nrespectively. Weight decay is set as 0.0005, and momentum \nis set as 0.9. Multi-scale training from 320 to 608 pixels is \napplied. MixUp[43] is adopted for data augmentation. <br><br>4.2. Ablation Study ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7fefe3ba-c364-4a08-90e3-6d5b8d548ffa": {"__data__": {"id_": "7fefe3ba-c364-4a08-90e3-6d5b8d548ffa", "embedding": null, "metadata": {"bbox": [{"page": 4, "page_height": 792.0, "page_width": 612.0, "x0": 308.86, "y0": 331.18, "x1": 545.12, "y1": 550.99}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "b53e6f20-453e-4bd7-8f79-1f19f8cb0965", "node_type": "1", "metadata": {"bbox": [{"page": 4, "page_height": 792.0, "page_width": 612.0, "x0": 50.11, "y0": 377.39, "x1": 286.37, "y1": 550.99}]}, "hash": "aea9f01fb7ef9dc8a15e5f07dd704aef57b8607c924b543185f3afd9c4c16ee7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9f051edc-6a9a-4862-b274-ba4d6dc28b09", "node_type": "1", "metadata": {"bbox": [{"page": 4, "page_height": 792.0, "page_width": 612.0, "x0": 50.11, "y0": 78.85, "x1": 286.37, "y1": 369.52}]}, "hash": "8d90002a3be28ad5cee92a5f72eb73a767a1ba865c81ea4476c96b8dc663bcab", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d412ac7f-8469-483d-8f0d-99c38edf6148", "node_type": "4", "metadata": {"file_name": "pp_yolo.pdf", "file_size": 500459, "creation_date": "2024-07-23", "last_modified_date": "2024-07-23"}, "hash": "2edaafdd30fe7c744e3ab970a04ef080547384dd694c611de46e66a2183187cc", "class_name": "RelatedNodeInfo"}}, "text": "a larger batch size and EMA to improve the stability of the \nmodel, and also apply DropBlock to prevent the model from \nover\ufb01tting. After using these strategies, the mAP of model \n(C) increases to 41.4% without any loss of ef\ufb01ciency. <br><br>C \u2192 F Next, we consider modifying the YOLO loss to im- \nprove the effectiveness of the model, because modifying the \nloss generally only has an impact on the training process, \nand will not or rarely affect the infer time. We add IoU \nLoss (D), IoU Aware (E) and Grid Sensitive (F) modules, \nand increase the mAP by 0.5%, 0.6% and 0.3% respec- \ntively. Among them, IoU loss will not affect the number \nof parameters and the infer time at all. IoU Aware and Grid \nSensitive will increase the post-processing time by 0.7ms \nand 0.1ms, since the current implementation is not ef\ufb01cient \nenough, which can be greatly reduced by merging them as \na single OP in PaddlePaddle in the future. On the whole, \nwe have increased the mAP of PP-YOLO from 41.4% to \n42.8%. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9f051edc-6a9a-4862-b274-ba4d6dc28b09": {"__data__": {"id_": "9f051edc-6a9a-4862-b274-ba4d6dc28b09", "embedding": null, "metadata": {"bbox": [{"page": 4, "page_height": 792.0, "page_width": 612.0, "x0": 50.11, "y0": 78.85, "x1": 286.37, "y1": 369.52}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "7fefe3ba-c364-4a08-90e3-6d5b8d548ffa", "node_type": "1", "metadata": {"bbox": [{"page": 4, "page_height": 792.0, "page_width": 612.0, "x0": 308.86, "y0": 331.18, "x1": 545.12, "y1": 550.99}]}, "hash": "1cd5a4cce42e5e75a9b258d38c916bd5d7724096bd252e7a43e955af3726ceef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fc20c5b1-4364-4497-9459-65971e85e9f0", "node_type": "1", "metadata": {"bbox": [{"page": 4, "page_height": 792.0, "page_width": 612.0, "x0": 308.86, "y0": 78.85, "x1": 545.12, "y1": 322.79}]}, "hash": "20f9713c8948b328412f6456f98bedfe3618729a433c74ab33be9ceb72d39ad9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d412ac7f-8469-483d-8f0d-99c38edf6148", "node_type": "4", "metadata": {"file_name": "pp_yolo.pdf", "file_size": 500459, "creation_date": "2024-07-23", "last_modified_date": "2024-07-23"}, "hash": "2edaafdd30fe7c744e3ab970a04ef080547384dd694c611de46e66a2183187cc", "class_name": "RelatedNodeInfo"}}, "text": "In this section, we present the effectiveness of each mod- \nule in an incremental manner. The reason is that each trick is \nnot completely independent. Some tricks are effective when \napplied alone, but they are not effective when combined to- \ngether. Since there are too many combinations of various \ntricks, it is dif\ufb01cult to conduct a comprehensive analysis. \nTherefore, we show how to improve the performance of \nthe object detector step by step in the order of our explo- \nration and discovering the effectiveness of tricks. Results \nare shown in Table 1, where infer time and FPS do not con- \nsider the in\ufb02uence of NMS following YOLOv4[1]. \nA \u2192 B First of all, we try to build a basic version of PP- \nYOLO. Because the ResNet[13] series is more widely used, \nwe \ufb01rst replace the original YOLOv3 backbone Darknet53 \nwith ResNet50-vd. However, we found that it will cause \na signi\ufb01cant decrease in mAP. Considering that the number \nof parameters and FLOPs of ResNet50-vd are much smaller \nthan those of Darknet53, we replace the 3 \u00d7 3 convolutional \nlayer in the last stage of ResNet with deformable convolu- \ntion layer[6]. In this way, we get a basic PP-YOLO model \n(B) with a mAP of 39.1%, which is slightly higher than the \noriginal YOLOv3 (A), but its parameters, FLOPs and infer \ntime are much smaller than the original YOLOv3 model. \nB \u2192 C We \ufb01rst try to optimize the training strategy. We use ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fc20c5b1-4364-4497-9459-65971e85e9f0": {"__data__": {"id_": "fc20c5b1-4364-4497-9459-65971e85e9f0", "embedding": null, "metadata": {"bbox": [{"page": 4, "page_height": 792.0, "page_width": 612.0, "x0": 308.86, "y0": 78.85, "x1": 545.12, "y1": 322.79}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "9f051edc-6a9a-4862-b274-ba4d6dc28b09", "node_type": "1", "metadata": {"bbox": [{"page": 4, "page_height": 792.0, "page_width": 612.0, "x0": 50.11, "y0": 78.85, "x1": 286.37, "y1": 369.52}]}, "hash": "8d90002a3be28ad5cee92a5f72eb73a767a1ba865c81ea4476c96b8dc663bcab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f24bd502-5279-45ef-8630-d6a8a3fde157", "node_type": "1", "metadata": {"bbox": [{"page": 5, "page_height": 792.0, "page_width": 612.0, "x0": 352.94, "y0": 654.74, "x1": 534.29, "y1": 713.9}]}, "hash": "f19c01fb17be39f09e4c8f3555d8a454ce1e73ae49b7583f2036c32c2a0cad3a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d412ac7f-8469-483d-8f0d-99c38edf6148", "node_type": "4", "metadata": {"file_name": "pp_yolo.pdf", "file_size": 500459, "creation_date": "2024-07-23", "last_modified_date": "2024-07-23"}, "hash": "2edaafdd30fe7c744e3ab970a04ef080547384dd694c611de46e66a2183187cc", "class_name": "RelatedNodeInfo"}}, "text": "F \u2192 G Post-processing is also a place where we can im- \nprove the performance. We use Matrix NMS (G) to replace \ntraditional greedy NMS. We can see that the mAP has im- \nproved by 0.6%. Since the infer time in Table 1 does not \nconsider NMS, so the in\ufb02uence is not shown here. In fact, \nthe overall infer time is decreased since the ef\ufb01ciency of \nMatrixNMS is higher than traditional NMS. <br><br>G \u2192 I It has become dif\ufb01cult to continue to improve mAP \nwithout increasing the number of parameters and FLOPs. \nSo we considered two methods that only increase a few \nparameters and FLOPs but can bring effective improve- \nments, CoordConv (H) and SPP (I). CoordConv will cause \nthe input channel of convolutional layers increase by 2, \nthe number of parameters increases by 0.03M, and FLOPs \nincreases by 0.05G, which is very small compared to the \nwhole model. It can bring an improvement of 0.5% mAP. \nSPP itself does not increase the parameters, but it will in- \ncrease the input channel of the convolutional layer just fol- \nlowing it, resulting in an increase of the parameters by 1M \nand FLOPs by 0.36G. It can improve the mAP of PP-YOLO ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f24bd502-5279-45ef-8630-d6a8a3fde157": {"__data__": {"id_": "f24bd502-5279-45ef-8630-d6a8a3fde157", "embedding": null, "metadata": {"bbox": [{"page": 5, "page_height": 792.0, "page_width": 612.0, "x0": 352.94, "y0": 654.74, "x1": 534.29, "y1": 713.9}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "fc20c5b1-4364-4497-9459-65971e85e9f0", "node_type": "1", "metadata": {"bbox": [{"page": 4, "page_height": 792.0, "page_width": 612.0, "x0": 308.86, "y0": 78.85, "x1": 545.12, "y1": 322.79}]}, "hash": "20f9713c8948b328412f6456f98bedfe3618729a433c74ab33be9ceb72d39ad9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "71e7ccb2-8827-41b5-8556-6c528a3f50c5", "node_type": "1", "metadata": {"bbox": [{"page": 5, "page_height": 792.0, "page_width": 612.0, "x0": 57.63, "y0": 470.36, "x1": 216.1, "y1": 695.44}]}, "hash": "0b084cbd05783c23c2e7d6a2675ea9ac81a41b694629b6ab509d50fd1b591ebe", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d412ac7f-8469-483d-8f0d-99c38edf6148", "node_type": "4", "metadata": {"file_name": "pp_yolo.pdf", "file_size": 500459, "creation_date": "2024-07-23", "last_modified_date": "2024-07-23"}, "hash": "2edaafdd30fe7c744e3ab970a04ef080547384dd694c611de46e66a2183187cc", "class_name": "RelatedNodeInfo"}}, "text": "AP  AP50  AP75  APS  APM  APL <br><br>- \n- \n- \n-  - \n- \n- \n-  - \n- \n- \n-  - \n- \n- \n-  - \n- \n- \n- ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "71e7ccb2-8827-41b5-8556-6c528a3f50c5": {"__data__": {"id_": "71e7ccb2-8827-41b5-8556-6c528a3f50c5", "embedding": null, "metadata": {"bbox": [{"page": 5, "page_height": 792.0, "page_width": 612.0, "x0": 57.63, "y0": 470.36, "x1": 216.1, "y1": 695.44}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "f24bd502-5279-45ef-8630-d6a8a3fde157", "node_type": "1", "metadata": {"bbox": [{"page": 5, "page_height": 792.0, "page_width": 612.0, "x0": 352.94, "y0": 654.74, "x1": 534.29, "y1": 713.9}]}, "hash": "f19c01fb17be39f09e4c8f3555d8a454ce1e73ae49b7583f2036c32c2a0cad3a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "26cb0032-b0d0-465d-a125-536f6164d65b", "node_type": "1", "metadata": {"bbox": [{"page": 5, "page_height": 792.0, "page_width": 612.0, "x0": 347.41, "y0": 470.36, "x1": 537.61, "y1": 695.44}]}, "hash": "399b128e1007a9e13e2e33bdb8e6e51df95b1f19d316b8840f803fe6ef296d9e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d412ac7f-8469-483d-8f0d-99c38edf6148", "node_type": "4", "metadata": {"file_name": "pp_yolo.pdf", "file_size": 500459, "creation_date": "2024-07-23", "last_modified_date": "2024-07-23"}, "hash": "2edaafdd30fe7c744e3ab970a04ef080547384dd694c611de46e66a2183187cc", "class_name": "RelatedNodeInfo"}}, "text": "ResNet-50 \nRetinaNet [22] \nResNet-101 \nRetinaNet [22] \nResNet-50 \nRetinaNet [22] \nResNet-101 \nRetinaNet [22] \nEf\ufb01cient-B0 \nEf\ufb01cientDet-D0 [35] \nEf\ufb01cient-B1 \nEf\ufb01cientDet-D1 [35] \nEf\ufb01cient-B2 \nEf\ufb01cientDet-D2 [35] \nEf\ufb01cient-B3 \nEf\ufb01cientDet-D2 [35] \nHarDNet68 \nRFBNet[3] \nRFBNet[3] \nHarDNet85 \nYOLOv3 + ASFF*[26] Darknet-53 \nYOLOv3 + ASFF*[26] Darknet-53 \nYOLOv3 + ASFF*[26] Darknet-53 \nYOLOv3 + ASFF*[26] Darknet-53 \nYOLOv4 [1] \nYOLOv4 [1] \nYOLOv4 [1] \nPP-YOLO \nPP-YOLO \nPP-YOLO \nPP-YOLO  CSPDarknet-53 \nCSPDarknet-53 \nCSPDarknet-53 \nResNet50-vd-dcn \nResNet50-vd-dcn \nResNet50-vd-dcn \nResNet50-vd-dcn ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "26cb0032-b0d0-465d-a125-536f6164d65b": {"__data__": {"id_": "26cb0032-b0d0-465d-a125-536f6164d65b", "embedding": null, "metadata": {"bbox": [{"page": 5, "page_height": 792.0, "page_width": 612.0, "x0": 347.41, "y0": 470.36, "x1": 537.61, "y1": 695.44}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "71e7ccb2-8827-41b5-8556-6c528a3f50c5", "node_type": "1", "metadata": {"bbox": [{"page": 5, "page_height": 792.0, "page_width": 612.0, "x0": 57.63, "y0": 470.36, "x1": 216.1, "y1": 695.44}]}, "hash": "0b084cbd05783c23c2e7d6a2675ea9ac81a41b694629b6ab509d50fd1b591ebe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f60397cb-21cf-41fd-99ef-3048fbca4dcb", "node_type": "1", "metadata": {"bbox": [{"page": 5, "page_height": 792.0, "page_width": 612.0, "x0": 50.11, "y0": 426.8, "x1": 545.11, "y1": 719.21}]}, "hash": "f1b53bd25c2bb831ed6394967424d36bea2b2dbe2fed616143bd42386e224107", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d412ac7f-8469-483d-8f0d-99c38edf6148", "node_type": "4", "metadata": {"file_name": "pp_yolo.pdf", "file_size": 500459, "creation_date": "2024-07-23", "last_modified_date": "2024-07-23"}, "hash": "2edaafdd30fe7c744e3ab970a04ef080547384dd694c611de46e66a2183187cc", "class_name": "RelatedNodeInfo"}}, "text": "37.0% \n37.9% \n40.1% \n41.1% \n33.8% 52.2% 35.8% 12.0% 38.3% 51.2% \n39.6% 58.6% 42.3% 17.9% 44.3% 56.0% \n43.0% 62.3% 46.2% 22.5% 47.0% 58.4% \n45.8% 65.0% 49.3% 26.6% 49.4% 59.8% \n33.9% 54.3% 36.2% 14.7% 36.6% 50.5% \n36.8% 57.1% 39.5% 16.9% 40.5% 52.9% \n38.1% 57.4% 42.1% 16.1% 41.6% 53.6% \n40.6% 60.6% 45.1% 20.3% 44.2% 54.1% \n42.4% 63.0% 47.4% 25.5% 45.7% 52.3% \n43.9% 64.1% 49.2% 27.0% 46.6% 53.4% \n41.2% 62.8% 44.3% 20.4% 44.4% 56.0% \n43.0% 64.9% 46.5% 24.3% 46.1% 55.2% \n43.5% 65.7% 47.3% 26.7% 46.7% 53.3% \n39.3% 59.3% 42.7% 16.7% 41.4% 57.8% \n42.5% 62.8% 46.5% 21.2% 45.2% 58.2% \n44.4% 64.6% 48.8% 24.4% 47.1% 58.2% \n45.2% 65.2% 49.9% 26.3% 47.8% 57.2% ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f60397cb-21cf-41fd-99ef-3048fbca4dcb": {"__data__": {"id_": "f60397cb-21cf-41fd-99ef-3048fbca4dcb", "embedding": null, "metadata": {"bbox": [{"page": 5, "page_height": 792.0, "page_width": 612.0, "x0": 50.11, "y0": 426.8, "x1": 545.11, "y1": 719.21}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "26cb0032-b0d0-465d-a125-536f6164d65b", "node_type": "1", "metadata": {"bbox": [{"page": 5, "page_height": 792.0, "page_width": 612.0, "x0": 347.41, "y0": 470.36, "x1": 537.61, "y1": 695.44}]}, "hash": "399b128e1007a9e13e2e33bdb8e6e51df95b1f19d316b8840f803fe6ef296d9e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "41d75903-1437-4372-a559-e6af1f0d8933", "node_type": "1", "metadata": {"bbox": [{"page": 5, "page_height": 792.0, "page_width": 612.0, "x0": 50.11, "y0": 78.85, "x1": 286.37, "y1": 404.72}]}, "hash": "2f636b54ab3423160a51900d2383914e3db03197835670195d6cc35665668249", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d412ac7f-8469-483d-8f0d-99c38edf6148", "node_type": "4", "metadata": {"file_name": "pp_yolo.pdf", "file_size": 500459, "creation_date": "2024-07-23", "last_modified_date": "2024-07-23"}, "hash": "2edaafdd30fe7c744e3ab970a04ef080547384dd694c611de46e66a2183187cc", "class_name": "RelatedNodeInfo"}}, "text": "FPS (V100) \nw/o TRT with TRT \nSize <br><br>640 \n640 \n1024 \n1024 \n512 \n640 \n768 \n896 \n512 \n512 \n320 \n416 \n608 \n800 \n416 \n512 \n608 \n320 \n416 \n512 \n608  37 \n29.4 \n19.6 \n15.4 \n98.0+ \n74.1+ \n56.5+ \n34.5+ \n41.5 \n37.1 \n60 \n54 \n45.5 \n29.4 \n96 \n83 \n62 \n132.2 \n109.6 \n89.9 \n72.9  - \n- \n- \n- \n- \n- \n- \n- \n- \n- \n- \n- \n- \n- \n164.0\u2217 \n138.4\u2217 \n105.5\u2217 \n242.2 \n215.4 \n188.4 \n155.6 <br><br>Table 2. Comparison of the speed and accuracy of different object detectors on the MS-COCO (test-dev 2017). We compare the results with \nbatch size = 1, without tensorRT (w/o TRT) or with tensorRT(with TRT). Results marked by \u201d+\u201d are updated results from the corresponding \nof\ufb01cial code base, which are higher than the results in original paper. Results marked by \u201d*\u201d are test in our environment using of\ufb01cial code \nand model, which are slightly higher than results reported in of\ufb01cial code-base. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "41d75903-1437-4372-a559-e6af1f0d8933": {"__data__": {"id_": "41d75903-1437-4372-a559-e6af1f0d8933", "embedding": null, "metadata": {"bbox": [{"page": 5, "page_height": 792.0, "page_width": 612.0, "x0": 50.11, "y0": 78.85, "x1": 286.37, "y1": 404.72}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "f60397cb-21cf-41fd-99ef-3048fbca4dcb", "node_type": "1", "metadata": {"bbox": [{"page": 5, "page_height": 792.0, "page_width": 612.0, "x0": 50.11, "y0": 426.8, "x1": 545.11, "y1": 719.21}]}, "hash": "f1b53bd25c2bb831ed6394967424d36bea2b2dbe2fed616143bd42386e224107", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d8e5c289-bf4c-444c-be55-a414314bfd23", "node_type": "1", "metadata": {"bbox": [{"page": 5, "page_height": 792.0, "page_width": 612.0, "x0": 308.86, "y0": 78.85, "x1": 545.12, "y1": 404.72}]}, "hash": "823cb5409e8eea59fca1869bd01ae2d41b9b30a824f64e7af88a37077de35648", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d412ac7f-8469-483d-8f0d-99c38edf6148", "node_type": "4", "metadata": {"file_name": "pp_yolo.pdf", "file_size": 500459, "creation_date": "2024-07-23", "last_modified_date": "2024-07-23"}, "hash": "2edaafdd30fe7c744e3ab970a04ef080547384dd694c611de46e66a2183187cc", "class_name": "RelatedNodeInfo"}}, "text": "by 0.3% further. After adding these two modules, the infer \ntime has increased by 0.3ms. \nI \u2192 J Replacing the pre-trained model is a very common \napproach. However, the accuracy of pretrained classi\ufb01ca- \ntion model is higher does not mean that the \ufb01nal detection \nmodel is more effective, and the degree of improvement will \nbe affected by the tricks we used. So we consider it at the \nend. For fair comparisons, we still use ImageNet for pre- \ntraining. We use a distilled ResNet50-vd model for back- \nbone initialization. The mAP of PP-YOLO can be further \nimproved by 0.3%. In fact, using other detection datasets \nfor pre-training can greatly improve the performance of the \nmodel, but this is beyond the scope of this paper. <br><br>4.3. Comparison with Other State-of-the-Art De- \ntectors <br><br>Comparison of the results on MS-COCO test split with \nother state-of-the-art object detectors are shown in Figure \n1 and Table 2. The FPS results of PP-YOLO and other \nmethods are all tested on V100 with batch size = 1. We \nconsidered two different test conditions, without tensorRT \n(w/o TRT) and with tensorRT (with TRT). The test methods \nare consistent with YOLOv4[1]. Results marked by \u201d+\u201d are \nupdated results from the corresponding of\ufb01cial code-base, \nwhich are higher than the results in original paper, Results \nmarked by \u201d*\u201d are test in our environment using of\ufb01cial \ncode and model, which are slightly higher than results re- ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d8e5c289-bf4c-444c-be55-a414314bfd23": {"__data__": {"id_": "d8e5c289-bf4c-444c-be55-a414314bfd23", "embedding": null, "metadata": {"bbox": [{"page": 5, "page_height": 792.0, "page_width": 612.0, "x0": 308.86, "y0": 78.85, "x1": 545.12, "y1": 404.72}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "41d75903-1437-4372-a559-e6af1f0d8933", "node_type": "1", "metadata": {"bbox": [{"page": 5, "page_height": 792.0, "page_width": 612.0, "x0": 50.11, "y0": 78.85, "x1": 286.37, "y1": 404.72}]}, "hash": "2f636b54ab3423160a51900d2383914e3db03197835670195d6cc35665668249", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "869a1af4-b767-4a76-a861-65b3abeca35e", "node_type": "1", "metadata": {"bbox": [{"page": 6, "page_height": 792.0, "page_width": 612.0, "x0": 308.86, "y0": 542.84, "x1": 545.12, "y1": 718.08}]}, "hash": "bf3a378d1a54fc774f0dc3630672f416be48ce04606063b8b776dfbbc99656eb", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d412ac7f-8469-483d-8f0d-99c38edf6148", "node_type": "4", "metadata": {"file_name": "pp_yolo.pdf", "file_size": 500459, "creation_date": "2024-07-23", "last_modified_date": "2024-07-23"}, "hash": "2edaafdd30fe7c744e3ab970a04ef080547384dd694c611de46e66a2183187cc", "class_name": "RelatedNodeInfo"}}, "text": "ported in of\ufb01cial code-base. <br><br>Compared with other state-of-the-art methods, our PP- \nYOLO has certain advantages in speed and accuracy. For \nexample, compared with YOLOv4, our PPYOLO can in- \ncreased the mAP on COCO from 43.5% to 45.2% with FPS \nimproved from 62 to 72.9. It is worth noticing that tensorRT \naccelerates the PP-YOLO model more obviously. The rel- \native improvement of PP-YOLO (around 100%) is larger \nthan YOLOv4(around 70%). We speculate that it is mainly \nbecause tensorRT optimizes for ResNet model better than \nDarknet. <br><br>In addition, we can get a series of PP-YOLO results by \nchanging the input size of the image. Here we also show \nthe results for 320, 416, 512 and 608 input sizes. Figure 1 \nshows that PP-YOLO results have advantages in the balance \nof speed and accuracy compared with other detectors. <br><br>5. Conclusions <br><br>This paper introduce a new implementation of object \ndetector based on PaddlePaddle, called PP-YOLO. PP- \nYOLO is faster (FPS) and more accurate(COCO mAP) than \nother state-of-the-art detectors, such as Ef\ufb01cientDet and \nYOLOv4. \nIn this paper, we explore a lot of tricks and \nshow how to combine these tricks on the YOLOv3 detec- \ntor and demonstrate their effectiveness. We hope this paper \ncan help developers and researchers save exploration time \nand get better performance in practical applications. ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "869a1af4-b767-4a76-a861-65b3abeca35e": {"__data__": {"id_": "869a1af4-b767-4a76-a861-65b3abeca35e", "embedding": null, "metadata": {"bbox": [{"page": 6, "page_height": 792.0, "page_width": 612.0, "x0": 308.86, "y0": 542.84, "x1": 545.12, "y1": 718.08}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "d8e5c289-bf4c-444c-be55-a414314bfd23", "node_type": "1", "metadata": {"bbox": [{"page": 5, "page_height": 792.0, "page_width": 612.0, "x0": 308.86, "y0": 78.85, "x1": 545.12, "y1": 404.72}]}, "hash": "823cb5409e8eea59fca1869bd01ae2d41b9b30a824f64e7af88a37077de35648", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4aee76d6-d5b9-4cdb-8eb1-057f78bc595e", "node_type": "1", "metadata": {"bbox": [{"page": 6, "page_height": 792.0, "page_width": 612.0, "x0": 55.09, "y0": 497.2, "x1": 286.37, "y1": 698.42}]}, "hash": "c610e2b6d6a4df86ff50847f82fde3b87e8611feb626fbb127f6cc5d72ff9861", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d412ac7f-8469-483d-8f0d-99c38edf6148", "node_type": "4", "metadata": {"file_name": "pp_yolo.pdf", "file_size": 500459, "creation_date": "2024-07-23", "last_modified_date": "2024-07-23"}, "hash": "2edaafdd30fe7c744e3ab970a04ef080547384dd694c611de46e66a2183187cc", "class_name": "RelatedNodeInfo"}}, "text": "the IEEE conference on computer vision and pattern \nrecognition, pages 770\u2013778, 2016. 1, 2, 4, 5 <br><br>[14] L. Huang, Y. Yang, Y. Deng, and Y. Yu. Densebox: \nUnifying landmark localization with end to end object \ndetection. arXiv preprint arXiv:1509.04874, 2015. 2 \n[15] B. Jiang, R. Luo, J. Mao, T. Xiao, and Y. Jiang. Ac- \nquisition of localization con\ufb01dence for accurate object \ndetection. In ECCV, pages 784\u2013799, 2018. 1, 2 \n[16] W. Ke, T. Zhang, Z. Huang, Q. Ye, J. Liu, and \nD. Huang. Multiple anchor learning for visual object \ndetection. arXiv preprint arXiv:1912.02252, 2019. 2 \n[17] T. Kong, F. Sun, H. Liu, Y. Jiang, and J. Shi. Fove- \nabox: Beyond anchor-based object detector. arXiv \npreprint arXiv:1904.03797, 2019. 2 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4aee76d6-d5b9-4cdb-8eb1-057f78bc595e": {"__data__": {"id_": "4aee76d6-d5b9-4cdb-8eb1-057f78bc595e", "embedding": null, "metadata": {"bbox": [{"page": 6, "page_height": 792.0, "page_width": 612.0, "x0": 55.09, "y0": 497.2, "x1": 286.37, "y1": 698.42}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "869a1af4-b767-4a76-a861-65b3abeca35e", "node_type": "1", "metadata": {"bbox": [{"page": 6, "page_height": 792.0, "page_width": 612.0, "x0": 308.86, "y0": 542.84, "x1": 545.12, "y1": 718.08}]}, "hash": "bf3a378d1a54fc774f0dc3630672f416be48ce04606063b8b776dfbbc99656eb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2b11a0b4-1068-42c1-b8ba-552879a746ab", "node_type": "1", "metadata": {"bbox": [{"page": 6, "page_height": 792.0, "page_width": 612.0, "x0": 308.86, "y0": 329.98, "x1": 545.12, "y1": 538.44}]}, "hash": "0450679e4ffe807cff5bae8925e950a2ad56513c39869411ad6e512814a33cc2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d412ac7f-8469-483d-8f0d-99c38edf6148", "node_type": "4", "metadata": {"file_name": "pp_yolo.pdf", "file_size": 500459, "creation_date": "2024-07-23", "last_modified_date": "2024-07-23"}, "hash": "2edaafdd30fe7c744e3ab970a04ef080547384dd694c611de46e66a2183187cc", "class_name": "RelatedNodeInfo"}}, "text": "[1] A. Bochkovskiy, C.-Y. Wang, and H.-Y. M. Liao. \nYolov4: Optimal speed and accuracy of object detec- \ntion. arXiv preprint arXiv:2004.10934, 2020. 1, 2, 3, \n4, 5, 6 <br><br>[2] Z. Cai and N. Vasconcelos. Cascade r-cnn: Delving \ninto high quality object detection. In Proceedings of \nthe IEEE conference on computer vision and pattern \nrecognition, pages 6154\u20136162, 2018. 1, 2 <br><br>[3] P. Chao, C.-Y. Kao, Y.-S. Ruan, C.-H. Huang, and Y.- \nL. Lin. Hardnet: A low memory traf\ufb01c network. In \nProceedings of the IEEE international conference on \ncomputer vision, 2019. 6 <br><br>[4] J. Choi, D. Chun, H. Kim, and H.-J. Lee. Gaussian \nyolov3: An accurate and fast object detector using \nIn \nlocalization uncertainty for autonomous driving. \nIEEE ICCV, pages 502\u2013511, 2019. 1, 2 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2b11a0b4-1068-42c1-b8ba-552879a746ab": {"__data__": {"id_": "2b11a0b4-1068-42c1-b8ba-552879a746ab", "embedding": null, "metadata": {"bbox": [{"page": 6, "page_height": 792.0, "page_width": 612.0, "x0": 308.86, "y0": 329.98, "x1": 545.12, "y1": 538.44}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "4aee76d6-d5b9-4cdb-8eb1-057f78bc595e", "node_type": "1", "metadata": {"bbox": [{"page": 6, "page_height": 792.0, "page_width": 612.0, "x0": 55.09, "y0": 497.2, "x1": 286.37, "y1": 698.42}]}, "hash": "c610e2b6d6a4df86ff50847f82fde3b87e8611feb626fbb127f6cc5d72ff9861", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fd9bcaf7-1115-42f9-9be3-2c59822901c1", "node_type": "1", "metadata": {"bbox": [{"page": 6, "page_height": 792.0, "page_width": 612.0, "x0": 55.09, "y0": 278.06, "x1": 286.37, "y1": 491.23}]}, "hash": "fb704b3d6b5b0f1748b11478f3620bf38893bb0e126f1a32049b11ea09866a79", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d412ac7f-8469-483d-8f0d-99c38edf6148", "node_type": "4", "metadata": {"file_name": "pp_yolo.pdf", "file_size": 500459, "creation_date": "2024-07-23", "last_modified_date": "2024-07-23"}, "hash": "2edaafdd30fe7c744e3ab970a04ef080547384dd694c611de46e66a2183187cc", "class_name": "RelatedNodeInfo"}}, "text": "[18] A. Krizhevsky, I. Sutskever, and G. E. Hinton. <br><br>Im- \nagenet classi\ufb01cation with deep convolutional neural \nnetworks. In Advances in neural information process- \ning systems, pages 1097\u20131105, 2012. 1 <br><br>[19] H. Law and J. Deng. Cornernet: Detecting objects \nas paired keypoints. In Proceedings of the European \nConference on Computer Vision (ECCV), pages 734\u2013 \n750, 2018. 2 <br><br>[20] Y. Li, Y. Chen, N. Wang, and Z. Zhang. Scale-aware \ntrident networks for object detection. In Proceedings \nof the IEEE International Conference on Computer Vi- \nsion, 2019. 1, 2 <br><br>[21] T.-Y. Lin, P. Doll\u00b4ar, R. Girshick, K. He, B. Hariharan, \nand S. Belongie. Feature pyramid networks for object \ndetection. In Proceedings of the IEEE conference on \ncomputer vision and pattern recognition, pages 2117\u2013 \n2125, 2017. 1, 2, 3 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fd9bcaf7-1115-42f9-9be3-2c59822901c1": {"__data__": {"id_": "fd9bcaf7-1115-42f9-9be3-2c59822901c1", "embedding": null, "metadata": {"bbox": [{"page": 6, "page_height": 792.0, "page_width": 612.0, "x0": 55.09, "y0": 278.06, "x1": 286.37, "y1": 491.23}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "2b11a0b4-1068-42c1-b8ba-552879a746ab", "node_type": "1", "metadata": {"bbox": [{"page": 6, "page_height": 792.0, "page_width": 612.0, "x0": 308.86, "y0": 329.98, "x1": 545.12, "y1": 538.44}]}, "hash": "0450679e4ffe807cff5bae8925e950a2ad56513c39869411ad6e512814a33cc2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ddf143f4-3c60-495f-9ace-87d2336da29c", "node_type": "1", "metadata": {"bbox": [{"page": 6, "page_height": 792.0, "page_width": 612.0, "x0": 50.11, "y0": 78.85, "x1": 286.37, "y1": 272.09}]}, "hash": "ce152ead90ca88cd1a8ce2a2a1ebd37826651148e7c7e2ca5354ef3696d42e26", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d412ac7f-8469-483d-8f0d-99c38edf6148", "node_type": "4", "metadata": {"file_name": "pp_yolo.pdf", "file_size": 500459, "creation_date": "2024-07-23", "last_modified_date": "2024-07-23"}, "hash": "2edaafdd30fe7c744e3ab970a04ef080547384dd694c611de46e66a2183187cc", "class_name": "RelatedNodeInfo"}}, "text": "[5] J. Dai, Y. Li, K. He, and J. Sun. R-fcn: Object de- \ntection via region-based fully convolutional networks. \nIn Advances in neural information processing systems, \npages 379\u2013387, 2016. 1, 2 <br><br>[6] J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, \nand Y. Wei. Deformable convolutional networks. In \nProceedings of the IEEE international conference on \ncomputer vision, pages 764\u2013773, 2017. 5 <br><br>[7] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and \nL. Fei-Fei. Imagenet: A large-scale hierarchical im- \nage database. In 2009 IEEE conference on computer \nvision and pattern recognition, pages 248\u2013255. Ieee, \n2009. 5 <br><br>[8] K. Duan, S. Bai, L. Xie, H. Qi, Q. Huang, and Q. Tian. \nCenternet: Object detection with keypoint triplets. In \nProceedings of the IEEE International Conference on \nComputer Vision, 2019. 2 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ddf143f4-3c60-495f-9ace-87d2336da29c": {"__data__": {"id_": "ddf143f4-3c60-495f-9ace-87d2336da29c", "embedding": null, "metadata": {"bbox": [{"page": 6, "page_height": 792.0, "page_width": 612.0, "x0": 50.11, "y0": 78.85, "x1": 286.37, "y1": 272.09}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "fd9bcaf7-1115-42f9-9be3-2c59822901c1", "node_type": "1", "metadata": {"bbox": [{"page": 6, "page_height": 792.0, "page_width": 612.0, "x0": 55.09, "y0": 278.06, "x1": 286.37, "y1": 491.23}]}, "hash": "fb704b3d6b5b0f1748b11478f3620bf38893bb0e126f1a32049b11ea09866a79", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0beb9e89-74ee-41ce-a029-9b0a6ae8ca90", "node_type": "1", "metadata": {"bbox": [{"page": 6, "page_height": 792.0, "page_width": 612.0, "x0": 308.86, "y0": 78.85, "x1": 545.12, "y1": 325.63}]}, "hash": "39473c91bf2b80118d78491e87c1d42b6b274a4dae1db858e08d3433d125b5af", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d412ac7f-8469-483d-8f0d-99c38edf6148", "node_type": "4", "metadata": {"file_name": "pp_yolo.pdf", "file_size": 500459, "creation_date": "2024-07-23", "last_modified_date": "2024-07-23"}, "hash": "2edaafdd30fe7c744e3ab970a04ef080547384dd694c611de46e66a2183187cc", "class_name": "RelatedNodeInfo"}}, "text": "[9] C.-Y. Fu, W. Liu, A. Ranga, A. Tyagi, and A. C. Berg. \nDSSD: Deconvolutional single shot detector. In arXiv \npreprint arXiv:1701.06659, 2016. 1, 2 <br><br>[10] G. Ghiasi, T.-Y. Lin, and Q. V. Le. Dropblock: A \nregularization method for convolutional networks. In \nNeurIPS, 2018. 3 <br><br>[11] R. B. Girshick. Fast R-CNN. In IEEE ICCV, pages \n1440\u20131448, 2015. 2 <br><br>[12] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyra- \nmid pooling in deep convolutional networks for visual \nIEEE transactions on pattern analysis \nrecognition. \nand machine intelligence, 37(9):1904\u20131916, 2015. 3, \n4 <br><br>[13] K. He, X. Zhang, S. Ren, and J. Sun. Deep resid- \nual learning for image recognition. In Proceedings of ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0beb9e89-74ee-41ce-a029-9b0a6ae8ca90": {"__data__": {"id_": "0beb9e89-74ee-41ce-a029-9b0a6ae8ca90", "embedding": null, "metadata": {"bbox": [{"page": 6, "page_height": 792.0, "page_width": 612.0, "x0": 308.86, "y0": 78.85, "x1": 545.12, "y1": 325.63}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "ddf143f4-3c60-495f-9ace-87d2336da29c", "node_type": "1", "metadata": {"bbox": [{"page": 6, "page_height": 792.0, "page_width": 612.0, "x0": 50.11, "y0": 78.85, "x1": 286.37, "y1": 272.09}]}, "hash": "ce152ead90ca88cd1a8ce2a2a1ebd37826651148e7c7e2ca5354ef3696d42e26", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "be3fab19-8c31-40db-a13d-3085db750beb", "node_type": "1", "metadata": {"bbox": [{"page": 7, "page_height": 792.0, "page_width": 612.0, "x0": 50.11, "y0": 521.34, "x1": 286.37, "y1": 717.85}]}, "hash": "c5f4091188b4b0cabd013d1b793d8b17545e60bddb7722a1e72299cca90d2e65", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d412ac7f-8469-483d-8f0d-99c38edf6148", "node_type": "4", "metadata": {"file_name": "pp_yolo.pdf", "file_size": 500459, "creation_date": "2024-07-23", "last_modified_date": "2024-07-23"}, "hash": "2edaafdd30fe7c744e3ab970a04ef080547384dd694c611de46e66a2183187cc", "class_name": "RelatedNodeInfo"}}, "text": "[22] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Doll\u00b4ar. \nFocal loss for dense object detection. In Proceedings \nof the IEEE international conference on computer vi- \nsion, pages 2980\u20132988, 2017. 1, 2, 6 <br><br>[23] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, \nD. Ramanan, P. Doll\u00b4ar, and C. L. Zitnick. Microsoft \ncoco: Common objects in context. In European con- \nference on computer vision, pages 740\u2013755. Springer, \n2014. 4 <br><br>[24] L. Liu, W. Ouyang, XiaogangWang, P. Fieguth, \nJ. Chen, X. Liu, and M. Pietikainen. Deep learning \nfor generic object detection: A survey. Int. J. Comp. \nVis., 2019. 2 <br><br>[25] R. Liu, J. Lehman, P. Molino, F. P. Such, E. Frank, \nA. Sergeev, and J. Yosinski. An intriguing failing of \nconvolutional neural networks and the coordconv so- \nlution. In NeurIPS, pages 9605\u20139616, 2018. 3, 4 \n[26] S. Liu, D. Huang, and Y. Wang. Learning spatial fu- \nsion for single-shot object detection. arXiv preprint \narXiv:1911.09516, 2019. 6 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "be3fab19-8c31-40db-a13d-3085db750beb": {"__data__": {"id_": "be3fab19-8c31-40db-a13d-3085db750beb", "embedding": null, "metadata": {"bbox": [{"page": 7, "page_height": 792.0, "page_width": 612.0, "x0": 50.11, "y0": 521.34, "x1": 286.37, "y1": 717.85}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "0beb9e89-74ee-41ce-a029-9b0a6ae8ca90", "node_type": "1", "metadata": {"bbox": [{"page": 6, "page_height": 792.0, "page_width": 612.0, "x0": 308.86, "y0": 78.85, "x1": 545.12, "y1": 325.63}]}, "hash": "39473c91bf2b80118d78491e87c1d42b6b274a4dae1db858e08d3433d125b5af", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1f8ee24c-7cad-4cb5-8941-e359bf51a9d6", "node_type": "1", "metadata": {"bbox": [{"page": 7, "page_height": 792.0, "page_width": 612.0, "x0": 308.86, "y0": 480.74, "x1": 545.12, "y1": 717.85}]}, "hash": "025060731a350cb3a8dff57e555f1507a98fa44563ca39c027e97d45afb08e8f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d412ac7f-8469-483d-8f0d-99c38edf6148", "node_type": "4", "metadata": {"file_name": "pp_yolo.pdf", "file_size": 500459, "creation_date": "2024-07-23", "last_modified_date": "2024-07-23"}, "hash": "2edaafdd30fe7c744e3ab970a04ef080547384dd694c611de46e66a2183187cc", "class_name": "RelatedNodeInfo"}}, "text": "[27] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, \nC.-Y. Fu, and A. C. Berg. Ssd: Single shot multibox \ndetector. In European conference on computer vision, \npages 21\u201337. Springer, 2016. 1 <br><br>[28] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. E. \nReed, C. Fu, and A. C. Berg. SSD: single shot multi- \nbox detector. In ECCV, pages 21\u201337, 2016. 1, 2 <br><br>[29] PaddleClas. \nIntroduction of model compres- \nhttps://github. \n[EB/OL]. <br><br>sion methods. \ncom/PaddlePaddle/PaddleClas/blob/ \nmaster/docs/en/advanced_tutorials/ \ndistillation/distillation_en.md. 4 \n[30] J. Redmon, S. K. Divvala, R. B. Girshick, and \nA. Farhadi. You only look once: Uni\ufb01ed, real-time ob- \nject detection. In IEEE CVPR, pages 779\u2013788, 2016. \n1, 2 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1f8ee24c-7cad-4cb5-8941-e359bf51a9d6": {"__data__": {"id_": "1f8ee24c-7cad-4cb5-8941-e359bf51a9d6", "embedding": null, "metadata": {"bbox": [{"page": 7, "page_height": 792.0, "page_width": 612.0, "x0": 308.86, "y0": 480.74, "x1": 545.12, "y1": 717.85}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "be3fab19-8c31-40db-a13d-3085db750beb", "node_type": "1", "metadata": {"bbox": [{"page": 7, "page_height": 792.0, "page_width": 612.0, "x0": 50.11, "y0": 521.34, "x1": 286.37, "y1": 717.85}]}, "hash": "c5f4091188b4b0cabd013d1b793d8b17545e60bddb7722a1e72299cca90d2e65", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "35068ff1-cc9e-4b7b-bb8a-58f4d09b601a", "node_type": "1", "metadata": {"bbox": [{"page": 7, "page_height": 792.0, "page_width": 612.0, "x0": 50.11, "y0": 332.39, "x1": 286.37, "y1": 516.94}]}, "hash": "96cafbe4081dc1e380ec4e8a211b21e4afc07dea064f9065204497e92ed2d6c3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d412ac7f-8469-483d-8f0d-99c38edf6148", "node_type": "4", "metadata": {"file_name": "pp_yolo.pdf", "file_size": 500459, "creation_date": "2024-07-23", "last_modified_date": "2024-07-23"}, "hash": "2edaafdd30fe7c744e3ab970a04ef080547384dd694c611de46e66a2183187cc", "class_name": "RelatedNodeInfo"}}, "text": "[41] J. Yu, Y. Jiang, Z. Wang, Z. Cao, and T. Huang. Unit- \nbox: An advanced object detection network. In Pro- \nceedings of the 24th ACM international conference on \nMultimedia, pages 516\u2013520. ACM, 2016. 2 <br><br>[42] J. Yu, Y. Jiang, Z. Wang, Z. Cao, and T. Huang. Unit- \nbox: An advanced object detection network. In MM, \n2016. 3, 4 <br><br>[43] H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez- \nPaz. mixup: Beyond empirical risk minimization. In \nICLR, 2018. 1, 5 <br><br>[44] S. Zhang, L. Wen, X. Bian, Z. Lei, and S. Z. Li. \nSingle-shot re\ufb01nement neural network for object de- \nIn Proceedings of the IEEE Conference \ntection. \non Computer Vision and Pattern Recognition, pages \n4203\u20134212, 2018. 2 <br><br>[45] X. Zhang, F. Wan, C. Liu, R. Ji, and Q. Ye. Freean- \nchor: Learning to match anchors for visual object de- \ntection. In Advances in neural information processing \nsystems, 2019. 1, 2 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "35068ff1-cc9e-4b7b-bb8a-58f4d09b601a": {"__data__": {"id_": "35068ff1-cc9e-4b7b-bb8a-58f4d09b601a", "embedding": null, "metadata": {"bbox": [{"page": 7, "page_height": 792.0, "page_width": 612.0, "x0": 50.11, "y0": 332.39, "x1": 286.37, "y1": 516.94}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "1f8ee24c-7cad-4cb5-8941-e359bf51a9d6", "node_type": "1", "metadata": {"bbox": [{"page": 7, "page_height": 792.0, "page_width": 612.0, "x0": 308.86, "y0": 480.74, "x1": 545.12, "y1": 717.85}]}, "hash": "025060731a350cb3a8dff57e555f1507a98fa44563ca39c027e97d45afb08e8f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "16b7bf08-4ef5-4eb4-87fd-7c0adabd2295", "node_type": "1", "metadata": {"bbox": [{"page": 7, "page_height": 792.0, "page_width": 612.0, "x0": 308.86, "y0": 289.45, "x1": 545.12, "y1": 475.76}]}, "hash": "e8b712522fdf253d1ed641bfe0f875c445e90ae95808153e5fff5807cd499ba8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d412ac7f-8469-483d-8f0d-99c38edf6148", "node_type": "4", "metadata": {"file_name": "pp_yolo.pdf", "file_size": 500459, "creation_date": "2024-07-23", "last_modified_date": "2024-07-23"}, "hash": "2edaafdd30fe7c744e3ab970a04ef080547384dd694c611de46e66a2183187cc", "class_name": "RelatedNodeInfo"}}, "text": "[31] J. Redmon and A. Farhadi. Yolo9000: better, faster, \nIn Proceedings of the IEEE conference on \nstronger. \ncomputer vision and pattern recognition, pages 7263\u2013 \n7271, 2017. 1, 2 <br><br>[32] J. Redmon and A. Farhadi. Yolov3: An incremen- \ntal improvement. arXiv preprint arXiv:1804.02767, \n2018. 1, 2, 3, 4, 5 <br><br>[33] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: \nTowards real-time object detection with region pro- \nIn Advances in neural information \nposal networks. \nprocessing systems, pages 91\u201399, 2015. 1, 2 <br><br>[34] H. Rezato\ufb01ghi, N. Tsoi, J. Gwak, A. Sadeghian, \nI. Reid, and S. Savarese. Generalized intersection over \nunion: A metric and a loss for bounding box regres- \nsion. In CVPR, 2019. 4 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "16b7bf08-4ef5-4eb4-87fd-7c0adabd2295": {"__data__": {"id_": "16b7bf08-4ef5-4eb4-87fd-7c0adabd2295", "embedding": null, "metadata": {"bbox": [{"page": 7, "page_height": 792.0, "page_width": 612.0, "x0": 308.86, "y0": 289.45, "x1": 545.12, "y1": 475.76}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "35068ff1-cc9e-4b7b-bb8a-58f4d09b601a", "node_type": "1", "metadata": {"bbox": [{"page": 7, "page_height": 792.0, "page_width": 612.0, "x0": 50.11, "y0": 332.39, "x1": 286.37, "y1": 516.94}]}, "hash": "96cafbe4081dc1e380ec4e8a211b21e4afc07dea064f9065204497e92ed2d6c3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2a6f6e74-9e25-45fb-ae5a-041c357524b5", "node_type": "1", "metadata": {"bbox": [{"page": 7, "page_height": 792.0, "page_width": 612.0, "x0": 50.11, "y0": 141.26, "x1": 286.37, "y1": 327.99}]}, "hash": "d8d14791beb6be05ef4193fe2d9b53fc14aed147656fde8ba5106294f344d96f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d412ac7f-8469-483d-8f0d-99c38edf6148", "node_type": "4", "metadata": {"file_name": "pp_yolo.pdf", "file_size": 500459, "creation_date": "2024-07-23", "last_modified_date": "2024-07-23"}, "hash": "2edaafdd30fe7c744e3ab970a04ef080547384dd694c611de46e66a2183187cc", "class_name": "RelatedNodeInfo"}}, "text": "[46] Z. Zheng, P. Wang, W. Liu, J. Li, R. Ye, and \nD. Ren. Distance-iou loss: Faster and better learning \nfor bounding box regression. In AAAI, 2020. 4 \n[47] X. Zhou, J. Zhuo, and P. Krahenbuhl. Bottom-up ob- \nject detection by grouping extreme and center points. \nIn Proceedings of the IEEE Conference on Computer \nVision and Pattern Recognition, pages 850\u2013859, 2019. \n2 <br><br>[48] C. Zhu, F. Chen, Z. Shen, and M. Savvides. \narXiv preprint <br><br>Soft anchor-point object detection. \narXiv:1911.12448, 2019. 2 <br><br>[49] C. Zhu, Y. He, and M. Savvides. Feature selective \nanchor-free module for single-shot object detection. In \nThe IEEE Conference on Computer Vision and Pattern \nRecognition (CVPR), June 2019. 2 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2a6f6e74-9e25-45fb-ae5a-041c357524b5": {"__data__": {"id_": "2a6f6e74-9e25-45fb-ae5a-041c357524b5", "embedding": null, "metadata": {"bbox": [{"page": 7, "page_height": 792.0, "page_width": 612.0, "x0": 50.11, "y0": 141.26, "x1": 286.37, "y1": 327.99}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "16b7bf08-4ef5-4eb4-87fd-7c0adabd2295", "node_type": "1", "metadata": {"bbox": [{"page": 7, "page_height": 792.0, "page_width": 612.0, "x0": 308.86, "y0": 289.45, "x1": 545.12, "y1": 475.76}]}, "hash": "e8b712522fdf253d1ed641bfe0f875c445e90ae95808153e5fff5807cd499ba8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "322348e1-f834-4e51-90e1-dde4f2d4a2ae", "node_type": "1", "metadata": {"bbox": [{"page": 7, "page_height": 792.0, "page_width": 612.0, "x0": 50.11, "y0": 78.85, "x1": 286.37, "y1": 150.99}]}, "hash": "40ed4ee3d4f192752302737cb7dd1fbc82ca1d32d3b1daee5261f34ebe60a9ef", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d412ac7f-8469-483d-8f0d-99c38edf6148", "node_type": "4", "metadata": {"file_name": "pp_yolo.pdf", "file_size": 500459, "creation_date": "2024-07-23", "last_modified_date": "2024-07-23"}, "hash": "2edaafdd30fe7c744e3ab970a04ef080547384dd694c611de46e66a2183187cc", "class_name": "RelatedNodeInfo"}}, "text": "[35] M. Tan, R. Pang, and Q. V. Le. Ef\ufb01cientdet: Scalable \nand ef\ufb01cient object detection. In CVPR, 2020. 1, 2, 3, \n4, 6 <br><br>[36] Z. Tian, C. Shen, H. Chen, and T. He. Fcos: Fully con- \nvolutional one-stage object detection. In Proceedings \nof the IEEE International Conference on Computer Vi- \nsion, 2019. 2 <br><br>[37] J. Wang, K. Chen, S. Yang, C. C. Loy, and D. Lin. \nRegion proposal by guided anchoring. In IEEE CVPR, \npages 2965\u20132974, 2019. 1, 2 <br><br>[38] X. Wang, R. Zhang, T. Kong, L. Li, and C. Shen. \nSolov2: Dynamic, faster and stronger. arXiv preprint \narXiv:2003.10152, 2020. 3, 4 \n[39] S. Wu, X. Li, and X. Wang. \nIou-aware single-stage \nImage and ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "322348e1-f834-4e51-90e1-dde4f2d4a2ae": {"__data__": {"id_": "322348e1-f834-4e51-90e1-dde4f2d4a2ae", "embedding": null, "metadata": {"bbox": [{"page": 7, "page_height": 792.0, "page_width": 612.0, "x0": 50.11, "y0": 78.85, "x1": 286.37, "y1": 150.99}]}, "excluded_embed_metadata_keys": ["bbox"], "excluded_llm_metadata_keys": ["bbox"], "relationships": {"2": {"node_id": "2a6f6e74-9e25-45fb-ae5a-041c357524b5", "node_type": "1", "metadata": {"bbox": [{"page": 7, "page_height": 792.0, "page_width": 612.0, "x0": 50.11, "y0": 141.26, "x1": 286.37, "y1": 327.99}]}, "hash": "d8d14791beb6be05ef4193fe2d9b53fc14aed147656fde8ba5106294f344d96f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d412ac7f-8469-483d-8f0d-99c38edf6148", "node_type": "4", "metadata": {"file_name": "pp_yolo.pdf", "file_size": 500459, "creation_date": "2024-07-23", "last_modified_date": "2024-07-23"}, "hash": "2edaafdd30fe7c744e3ab970a04ef080547384dd694c611de46e66a2183187cc", "class_name": "RelatedNodeInfo"}}, "text": "object detector for accurate localization. \nVision Computing, page 103911, 2020. 3, 4 <br><br>[40] Z. Yang, S. Liu, H. Hu, L. Wang, and S. Lin. Rep- \npoints: Point set representation for object detection. \nIn Proceedings of the IEEE International Conference \non Computer Vision, 2019. 2 ", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"c53cf204-c498-4982-927d-a3a62b06cea1": {"doc_hash": "8bf90a5a4ade4e28510948bd49cd655177da0e6bf43c5fb0436fd8d6056d23d9"}, "24264475-13bb-491d-ba2f-e85d624a86e3": {"doc_hash": "d43cf790659b4f48142fc647f8b1110adf9e7ff99c32d1e97cd0ab647ccd9e0c"}, "a6ede9e9-9049-499f-8f31-091d8c3b3ea1": {"doc_hash": "f961a08e5d9e8743b94aa44254d18e4ae1c1febb5058de9eefcf64d538b4925a"}, "99a618ca-6910-4ecc-abfd-a08897c50dcf": {"doc_hash": "4577c44ab420bf356d0ffeca5d5bcd53c5c38e451624b95c1e1af9ec714cd8cc"}, "17679c76-e1e3-404b-95f2-4c90f9461333": {"doc_hash": "5c88c42c578780459f89857559c067fbbfa8cdfdcd8c0e227574086128028cb9"}, "e48927a0-e626-4817-8557-cab6665e57fb": {"doc_hash": "0b41bc1edf2843155d1e7c69943f73307a37b1e9233cb5e580e5a22d15096647"}, "d78092a9-0535-48fc-b636-0020d3b0158b": {"doc_hash": "b5fa0f842ec21e07e376d54718aa7016cff1c87dcf1fcb38d986fd3eb2149883"}, "646f4d3b-f991-45cb-af5d-fd143bfc2b5d": {"doc_hash": "40cdec27ddcf2f37cd607953da3ba9261ad801364cb158fc40171ad7d7aba71f"}, "7f4932ea-4ac6-47e9-bdfc-4342111a2e55": {"doc_hash": "2bf5f612ba6a6836afb44990c9f4eb50dca6d355cde1e281201b967f85f786e6"}, "5f9c19a5-bd59-48d3-854a-69543208130d": {"doc_hash": "d173ce222389ae1a7ea9fd29f0586d8519be73da1531698e9f49071d12b570d9"}, "2bccbb14-c4f9-438c-8da3-a1f8b6389ace": {"doc_hash": "59e6b5c0560d1d05d8e47ad6ede8a0acc3a5fce09f99740ca6981ee2c285c3eb"}, "0795b446-fb38-4554-86a3-0a2b6303a8ff": {"doc_hash": "3c2337f7dd764b69412171a00d08c2d72076e22b1fc446a7340a30166cfb4ccc"}, "67bead9e-5886-4dc4-ba75-9b6d723500cf": {"doc_hash": "460406655e194faa7f46f99fb862b2747f8e3f1f18952640b12c97f3ef03b836"}, "daf3014b-792c-4ec0-84de-d75464160b2b": {"doc_hash": "07a5deb1cb008766fde94b7282c1fefdfae17781c425a71159bdac5c9de338e9"}, "076e8afd-9d4b-4a39-9b8d-d304feac8cc4": {"doc_hash": "19f82c632033555c2a034516876b77cff14ea0bda47dbac89ec1345b882d7641"}, "690523a4-7491-45d9-a84a-6eddccdc2735": {"doc_hash": "9e26f9ea25f6c303d717ed5f44ec8549fc9a5cec00188c02c960ebb8bfe77c43"}, "231cf635-3d41-4468-b701-640e0056f324": {"doc_hash": "ec2adf705ba460730ecb5467830a2df3250e4ca3e7671b9527708664c8d4fdd1"}, "c5ea2aae-9077-431f-96db-46706e9f7d26": {"doc_hash": "f9c703093d9973154aac4dfdfb45cc7390ac22971ceda1dde4cae9adcc71c04e"}, "a10b8b19-c68d-4a05-924f-80a65d4e997d": {"doc_hash": "a547f115f058fc36baab324df0bfa2dc0d48a33f965ad2a7bdf431d3be111fa0"}, "b53e6f20-453e-4bd7-8f79-1f19f8cb0965": {"doc_hash": "aea9f01fb7ef9dc8a15e5f07dd704aef57b8607c924b543185f3afd9c4c16ee7"}, "7fefe3ba-c364-4a08-90e3-6d5b8d548ffa": {"doc_hash": "1cd5a4cce42e5e75a9b258d38c916bd5d7724096bd252e7a43e955af3726ceef"}, "9f051edc-6a9a-4862-b274-ba4d6dc28b09": {"doc_hash": "8d90002a3be28ad5cee92a5f72eb73a767a1ba865c81ea4476c96b8dc663bcab"}, "fc20c5b1-4364-4497-9459-65971e85e9f0": {"doc_hash": "20f9713c8948b328412f6456f98bedfe3618729a433c74ab33be9ceb72d39ad9"}, "f24bd502-5279-45ef-8630-d6a8a3fde157": {"doc_hash": "f19c01fb17be39f09e4c8f3555d8a454ce1e73ae49b7583f2036c32c2a0cad3a"}, "71e7ccb2-8827-41b5-8556-6c528a3f50c5": {"doc_hash": "0b084cbd05783c23c2e7d6a2675ea9ac81a41b694629b6ab509d50fd1b591ebe"}, "26cb0032-b0d0-465d-a125-536f6164d65b": {"doc_hash": "399b128e1007a9e13e2e33bdb8e6e51df95b1f19d316b8840f803fe6ef296d9e"}, "f60397cb-21cf-41fd-99ef-3048fbca4dcb": {"doc_hash": "f1b53bd25c2bb831ed6394967424d36bea2b2dbe2fed616143bd42386e224107"}, "41d75903-1437-4372-a559-e6af1f0d8933": {"doc_hash": "2f636b54ab3423160a51900d2383914e3db03197835670195d6cc35665668249"}, "d8e5c289-bf4c-444c-be55-a414314bfd23": {"doc_hash": "823cb5409e8eea59fca1869bd01ae2d41b9b30a824f64e7af88a37077de35648"}, "869a1af4-b767-4a76-a861-65b3abeca35e": {"doc_hash": "bf3a378d1a54fc774f0dc3630672f416be48ce04606063b8b776dfbbc99656eb"}, "4aee76d6-d5b9-4cdb-8eb1-057f78bc595e": {"doc_hash": "c610e2b6d6a4df86ff50847f82fde3b87e8611feb626fbb127f6cc5d72ff9861"}, "2b11a0b4-1068-42c1-b8ba-552879a746ab": {"doc_hash": "0450679e4ffe807cff5bae8925e950a2ad56513c39869411ad6e512814a33cc2"}, "fd9bcaf7-1115-42f9-9be3-2c59822901c1": {"doc_hash": "fb704b3d6b5b0f1748b11478f3620bf38893bb0e126f1a32049b11ea09866a79"}, "ddf143f4-3c60-495f-9ace-87d2336da29c": {"doc_hash": "ce152ead90ca88cd1a8ce2a2a1ebd37826651148e7c7e2ca5354ef3696d42e26"}, "0beb9e89-74ee-41ce-a029-9b0a6ae8ca90": {"doc_hash": "39473c91bf2b80118d78491e87c1d42b6b274a4dae1db858e08d3433d125b5af"}, "be3fab19-8c31-40db-a13d-3085db750beb": {"doc_hash": "c5f4091188b4b0cabd013d1b793d8b17545e60bddb7722a1e72299cca90d2e65"}, "1f8ee24c-7cad-4cb5-8941-e359bf51a9d6": {"doc_hash": "025060731a350cb3a8dff57e555f1507a98fa44563ca39c027e97d45afb08e8f"}, "35068ff1-cc9e-4b7b-bb8a-58f4d09b601a": {"doc_hash": "96cafbe4081dc1e380ec4e8a211b21e4afc07dea064f9065204497e92ed2d6c3"}, "16b7bf08-4ef5-4eb4-87fd-7c0adabd2295": {"doc_hash": "e8b712522fdf253d1ed641bfe0f875c445e90ae95808153e5fff5807cd499ba8"}, "2a6f6e74-9e25-45fb-ae5a-041c357524b5": {"doc_hash": "d8d14791beb6be05ef4193fe2d9b53fc14aed147656fde8ba5106294f344d96f"}, "322348e1-f834-4e51-90e1-dde4f2d4a2ae": {"doc_hash": "40ed4ee3d4f192752302737cb7dd1fbc82ca1d32d3b1daee5261f34ebe60a9ef"}}}